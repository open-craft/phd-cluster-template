{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#launchpad-documentation","title":"Launchpad Documentation","text":"<p>Welcome to the documentation for the Launchpad stack -  a solution for hosting Open edX instances on Kubernetes with GitOps-based deployment and automated instance management.</p>"},{"location":"#what-is-launchpad","title":"What is Launchpad?","text":"<p>Launchpad combines open-source tools to run and operate Open edX at scale:</p> <ul> <li>Picasso -  Builds Docker images for Open edX services via GitHub Actions</li> <li>Harmony -  Open edX Terraform and Kubernetes infrastructure (ingress, optional monitoring, backups)</li> <li>Drydock -  Tutor plugin for ArgoCD-friendly Kubernetes deployments</li> </ul> <p>Together with Kubernetes, ArgoCD, Argo Workflows, and Tutor, the stack supports multiple isolated Open edX instances per cluster, each with its own databases, storage, and configuration.</p>"},{"location":"#documentation-sections","title":"Documentation sections","text":"<p>The documentation is split into four major and some minor sections. The four major sections are the following.</p> Section Description Infrastructure Cluster components, provisioning, deprovisioning, and custom resources Cluster Operating a cluster: authentication, configuration, upgrade, backup, restore, monitoring Instances Open edX instance lifecycle: provisioning, configuration, images, scaling, logs, monitoring, debugging User Guides Task-based guides: AWS WAF/ALB, maintenance mode, cron jobs, PR sandboxes, multi-domain, private plugins"},{"location":"#whats-next","title":"What's next?","text":"<p>The Open edX platform and related tooling can be complex.</p> <p>To get started, it is recommended to read the \"Overview\" of all major sections to familiarize yourself with all the concepts, components, and reasoning behind choices.</p> <p>You can easily navigate there by clicking any of the section links in the table above.</p>"},{"location":"cluster/","title":"Overview","text":""},{"location":"cluster/#cluster-overview","title":"Cluster Overview","text":"<p>This section covers operations and configuration for a Launchpad Kubernetes cluster after it has been provisioned.</p>"},{"location":"cluster/#topics","title":"Topics","text":"<ul> <li>Authentication -  Cluster access, kubeconfig, and Argo user management</li> <li>Configuration -  Cluster-wide settings, environment variables, and Terraform variables</li> <li>Upgrade -  Upgrading cluster components, ArgoCD, Argo Workflows, and node pools</li> <li>Backup -  Backup strategies (e.g. Velero) and schedules</li> <li>Restore -  Restoring from backups</li> <li>Monitoring -  Prometheus, Grafana, and alerting</li> </ul>"},{"location":"cluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>A cluster created via Infrastructure Provisioning</li> <li><code>kubectl</code> and Launchpad CLI configured for the cluster</li> </ul>"},{"location":"cluster/#related-documentation","title":"Related Documentation","text":"<ul> <li>Introduction -  Documentation home</li> <li>Infrastructure Overview -  How the cluster is provisioned</li> <li>Infrastructure Provisioning -  Cluster creation and ArgoCD install</li> <li>Instances Overview -  Open edX instance lifecycle</li> </ul>"},{"location":"cluster/#see-also","title":"See Also","text":"<ul> <li>Authentication -  kubeconfig and Argo users</li> <li>Configuration -  Environment variables and Terraform</li> <li>Backup -  Backup strategies</li> <li>Monitoring -  Prometheus and Grafana</li> </ul>"},{"location":"cluster/authentication/","title":"Authentication","text":""},{"location":"cluster/authentication/#cluster-authentication","title":"Cluster Authentication","text":"<p>This page describes how to authenticate to the Kubernetes cluster and to ArgoCD/Argo Workflows.</p>"},{"location":"cluster/authentication/#kubernetes-access-kubeconfig","title":"Kubernetes Access (kubeconfig)","text":"<p>Cluster access is configured via kubeconfig, generated by Terraform. The kubeconfig is used by the Launchpad CLI and can be used for manual interaction with the cluster as well.</p> <p>To get the kubeconfig generated, run <code>tofu apply -target local_sensitive_file.kubeconfig</code>. The command will render the file in the <code>infrastructure</code> directory under the name of <code>.kubeconfig</code>.</p> <p>Example after provisioning:</p> <pre><code>export KUBECONFIG=/path/to/.kubeconfig\nkubectl get nodes\n</code></pre> <p>Another option for ensuring the Kubernetes access is provided is by running <code>source ./activate</code> in the cluster repository root. It will set the <code>KUBECONFIG</code> path. Make sure the <code>.kubeconfig</code> file is present before running the command.</p>"},{"location":"cluster/authentication/#argocd-users","title":"ArgoCD Users","text":"<p>Users for ArgoCD are created with the Launchpad CLI. See Creating ArgoCD Users in the Infrastructure Provisioning guide for:</p> <ul> <li><code>phd_create_argo_user</code> -  Create users with roles (admin, developer, readonly)</li> <li><code>phd_update_argo_user</code> -  Update role or password</li> <li><code>phd_delete_argo_user</code> -  Remove a user</li> </ul> <p>After creating or updating users, restart the ArgoCD server so changes take effect:</p> <pre><code>kubectl delete pod -n argocd -l app.kubernetes.io/name=argocd-server\n</code></pre>"},{"location":"cluster/authentication/#web-ui-access","title":"Web UI Access","text":"<ul> <li>ArgoCD: <code>https://argocd.&lt;cluster-domain&gt;</code></li> </ul> <p>Use the credentials created with <code>phd_create_argo_user</code> or the admin password set (or generated) during <code>phd_install_argo</code>.</p>"},{"location":"cluster/authentication/#related-documentation","title":"Related Documentation","text":"<ul> <li>Cluster Overview -  Cluster operations</li> <li>Infrastructure Provisioning -  Creating ArgoCD users</li> <li>Cluster Configuration -  Launchpad env vars and kubeconfig</li> <li>Instances Overview -  Instance access</li> </ul>"},{"location":"cluster/authentication/#see-also","title":"See Also","text":"<ul> <li>Infrastructure Overview -  ArgoCD and Argo Workflows</li> <li>Instance Provisioning -  Instance creation</li> <li>User Guides Overview -  Task-based guides</li> </ul>"},{"location":"cluster/backup/","title":"Backup","text":""},{"location":"cluster/backup/#cluster-backup","title":"Cluster Backup","text":"<p>This page describes backup options for the Launchpad cluster.</p>"},{"location":"cluster/backup/#overview","title":"Overview","text":"<p>Backups may include:</p> <ul> <li>Kubernetes resources -  Via Velero for cluster-scoped and namespaced resources</li> <li>Persistent volumes -  Snapshot by Velero or the cloud provider</li> <li>Databases -  Managed DB snapshots (RDS, DocumentDB, DigitalOcean managed DBs) or application-level dumps</li> <li>Object storage -  Versioning or replication on S3/Spaces, if configured</li> </ul>"},{"location":"cluster/backup/#velero-when-enabled","title":"Velero (when enabled)","text":"<p>If Velero was enabled during infrastructure provisioning (Harmony option), it can provide:</p> <ul> <li>Scheduled backups of cluster resources and volumes</li> <li>Backup to S3-compatible storage (e.g. S3, DigitalOcean Spaces)</li> </ul> <p>Typical steps:</p> <ol> <li>Verify Velero is installed: <code>kubectl get pods -n velero</code></li> <li>Check schedules: <code>velero schedule get</code> (if Velero CLI is configured)</li> <li>Create or adjust schedules via Terraform/Harmony configuration or Velero CRDs</li> </ol> <p>Backup location and schedules are defined in the Terraform variables (e.g. <code>velero_schedules</code>). For more information, check the Harmony documentation.</p>"},{"location":"cluster/backup/#database-backups","title":"Database Backups","text":"<ul> <li>Managed MySQL/MongoDB: Use cloud provider snapshot/backup features (e.g. RDS automated backups, DO managed DB backups). Configure retention and restore procedures in the provider console or Terraform.</li> <li>Application-level: For Open edX, consider <code>mysqldump</code> or MongoDB dump jobs in addition to managed backups.</li> </ul>"},{"location":"cluster/backup/#best-practices","title":"Best Practices","text":"<ul> <li>Document what is backed up (cluster state, PVs, DBs, object storage).</li> <li>Test restore procedures periodically.</li> <li>Keep credentials and backup storage access secure.</li> </ul>"},{"location":"cluster/backup/#related-documentation","title":"Related Documentation","text":"<ul> <li>Cluster Overview -  Cluster operations</li> <li>Restore -  Restoring from backups</li> <li>Infrastructure Provisioning -  Enabling Velero and backup bucket</li> <li>Infrastructure Deprovisioning -  Cleanup and backups</li> </ul>"},{"location":"cluster/backup/#see-also","title":"See Also","text":"<ul> <li>Cluster Configuration -  Terraform and variables</li> <li>Instance Provisioning -  Instance resources to back up</li> <li>Monitoring -  Metrics and alerting</li> </ul>"},{"location":"cluster/configuration/","title":"Configuration","text":""},{"location":"cluster/configuration/#cluster-configuration","title":"Cluster Configuration","text":"<p>This page summarizes cluster-wide configuration for the Launchpad stack.</p>"},{"location":"cluster/configuration/#launchpad-cli-environment-variables","title":"Launchpad CLI Environment Variables","text":"<p>Required for most Launchpad commands:</p> <ul> <li><code>LAUNCHPAD_CLUSTER_DOMAIN</code> -  Cluster domain (e.g. <code>prod.cluster.domain</code>)</li> </ul> <p>Optional, but highly recommended:</p> <ul> <li><code>LAUNCHPAD_DOCKER_REGISTRY_CREDENTIALS</code> -  Base64-encoded credentials for private image pulls</li> </ul> <p>Optional:</p> <ul> <li><code>LAUNCHPAD_ARGO_ADMIN_PASSWORD</code> -  Admin password for ArgoCD and Argo Workflows</li> <li><code>LAUNCHPAD_ARGOCD_VERSION</code> -  ArgoCD version (default: <code>stable</code>)</li> <li><code>LAUNCHPAD_ARGO_WORKFLOWS_VERSION</code> -  Argo Workflows version (default: <code>stable</code>)</li> <li><code>LAUNCHPAD_OPENCRAFT_MANIFESTS_URL</code> -  Base URL for OpenCraft manifests</li> <li><code>LAUNCHPAD_DOCKER_REGISTRY</code> -  Registry hostname (e.g. <code>ghcr.io</code>)</li> </ul>"},{"location":"cluster/configuration/#terraformopentofu-variables","title":"Terraform/OpenTofu Variables","text":"<p>Cluster infrastructure is configured via variables in the generated cluster repo (<code>/infrastructure</code>). See <code>variables.tf</code> in that directory for:</p> <ul> <li>Cloud provider and region</li> <li>Cluster name and domain</li> <li>Node pool sizing</li> <li>Database and storage settings</li> <li>Harmony options (monitoring, Velero, etc.)</li> </ul> <p>Backend credentials are typically provided via environment variables (e.g. <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>) rather than in <code>.tf</code> files.</p>"},{"location":"cluster/configuration/#related-documentation","title":"Related Documentation","text":"<ul> <li>Cluster Overview -  Cluster operations</li> <li>Infrastructure Provisioning -  Initial setup and required env vars</li> <li>Infrastructure Overview -  Core components</li> <li>Instance Configuration -  Instance-level configuration</li> </ul>"},{"location":"cluster/configuration/#see-also","title":"See Also","text":"<ul> <li>Authentication -  kubeconfig and user management</li> <li>Upgrade -  Upgrading components</li> <li>Cluster Monitoring -  Prometheus and Grafana config</li> </ul>"},{"location":"cluster/monitoring/","title":"Monitoring","text":""},{"location":"cluster/monitoring/#cluster-monitoring","title":"Cluster Monitoring","text":"<p>This page describes monitoring options for the Launchpad cluster.</p>"},{"location":"cluster/monitoring/#overview","title":"Overview","text":"<p>When enabled via Harmony/Terraform, the stack can include:</p> <ul> <li>Prometheus -  Metrics collection from the cluster and applications</li> <li>Grafana -  Dashboards and visualization</li> <li>Alertmanager -  Alert routing and notifications</li> </ul> <p>Configuration is typically done through Terraform variables (e.g. <code>prometheus_enabled</code>, <code>grafana_enabled</code>, <code>alertmanager_config</code>).</p>"},{"location":"cluster/monitoring/#accessing-grafana","title":"Accessing Grafana","text":"<p>If Grafana is enabled, it is usually exposed at a hostname such as <code>grafana.&lt;cluster-domain&gt;</code>. Check the Terraform outputs or ingress resources:</p> <pre><code>kubectl get ingress -A | grep grafana\n</code></pre> <p>Use the credentials configured in Terraform or stored in a Kubernetes secret (e.g. in the same namespace as Grafana).</p>"},{"location":"cluster/monitoring/#prometheus-and-alertmanager","title":"Prometheus and Alertmanager","text":"<ul> <li>Prometheus scrapes metrics from cluster and application targets. Queries can be run in Grafana or via the Prometheus UI if exposed.</li> <li>Alertmanager receives alerts from Prometheus and sends notifications (e.g. email, Slack). Configure receivers and routes in the Terraform/Alertmanager config.</li> </ul>"},{"location":"cluster/monitoring/#open-edx-instance-metrics","title":"Open edX Instance Metrics","text":"<p>Instance-level monitoring (e.g. LMS, Celery, Redis) may be documented under Instances \u2192 Monitoring. Ensure service monitors or scrape configs include the instance namespaces if you want them in cluster Prometheus.</p>"},{"location":"cluster/monitoring/#related-documentation","title":"Related Documentation","text":"<ul> <li>Cluster Overview -  Cluster operations</li> <li>Infrastructure Overview -  Harmony and optional monitoring stack</li> <li>Cluster Configuration -  Terraform and monitoring variables</li> <li>Instance Monitoring -  Instance-level monitoring</li> </ul>"},{"location":"cluster/monitoring/#see-also","title":"See Also","text":"<ul> <li>Backup -  Backup and retention</li> <li>Restore -  Restore procedures</li> <li>User Guides Overview -  Task-based guides</li> </ul>"},{"location":"cluster/restore/","title":"Restore","text":""},{"location":"cluster/restore/#cluster-restore","title":"Cluster Restore","text":"<p>This page describes how to restore cluster state and data from backups.</p>"},{"location":"cluster/restore/#overview","title":"Overview","text":"<p>Restore procedures depend on what was backed up:</p> <ul> <li>Velero -  Restore Kubernetes resources and optionally persistent volume snapshots</li> <li>Databases -  Restore from cloud provider snapshots or from application-level dumps</li> <li>Object storage -  Restore from versioning or replication if configured</li> </ul> <p>Always follow your organization\u2019s runbooks and test restores in a non-production environment when possible.</p>"},{"location":"cluster/restore/#velero-restore","title":"Velero Restore","text":"<p>If Velero is installed and backups exist:</p> <ol> <li>Install and configure the Velero CLI and point it at the backup storage.</li> <li>List backups: <code>velero backup get</code></li> <li>Restore a backup: <code>velero restore create --from-backup &lt;backup-name&gt;</code> (optionally with filters for namespaces or resources).</li> </ol> <p>Check the Velero documentation for filters, volume snapshot restores, and cluster-specific considerations.</p>"},{"location":"cluster/restore/#database-restore","title":"Database Restore","text":"<ul> <li>Managed MySQL/MongoDB: Use the cloud provider\u2019s restore feature (e.g. restore from RDS snapshot to a new instance, then point the app to it). Adjust connection strings or Kubernetes secrets as needed.</li> <li>Application-level dumps: Restore using <code>mysql</code> or <code>mongorestore</code> into the target database, then restart the application if necessary.</li> </ul>"},{"location":"cluster/restore/#after-restore","title":"After Restore","text":"<ul> <li>Verify ArgoCD applications and sync status.</li> <li>Verify Open edX instances and that DB connectivity and secrets are correct.</li> <li>Re-run any provisioning or init jobs if data or resources were recreated.</li> </ul>"},{"location":"cluster/restore/#related-documentation","title":"Related Documentation","text":"<ul> <li>Cluster Overview -  Cluster operations</li> <li>Backup -  What is backed up and how</li> <li>Infrastructure Provisioning -  Re-provisioning after restore</li> <li>Infrastructure Deprovisioning -  If you need to rebuild the cluster</li> </ul>"},{"location":"cluster/restore/#see-also","title":"See Also","text":"<ul> <li>Cluster Configuration -  Post-restore configuration</li> <li>Instance Provisioning -  Recreating instances</li> <li>Instance Deprovisioning -  Instance cleanup</li> </ul>"},{"location":"cluster/upgrade/","title":"Upgrade","text":""},{"location":"cluster/upgrade/#cluster-upgrade","title":"Cluster Upgrade","text":"<p>This page outlines how to upgrade cluster components in the Launchpad stack.</p>"},{"location":"cluster/upgrade/#overview","title":"Overview","text":"<p>Upgrades may involve:</p> <ul> <li>Kubernetes version -  Managed by the cloud provider (EKS/DOKS)</li> <li>ArgoCD -  Version is set via <code>LAUNCHPAD_ARGOCD_VERSION</code>; re-run or adjust install</li> <li>Argo Workflows -  Version is set via <code>LAUNCHPAD_ARGO_WORKFLOWS_VERSION</code></li> <li>Node pools -  Updated via Terraform/OpenTofu (e.g. AMI/image or node version)</li> <li>Harmony / Terraform modules -  Bump module refs in Terraform and apply</li> </ul> <p>Always review release notes and backup critical state before upgrading.</p>"},{"location":"cluster/upgrade/#argocd-and-argo-workflows","title":"ArgoCD and Argo Workflows","text":"<p>To change the version of ArgoCD or Argo Workflows:</p> <ol> <li>Set the desired version (e.g. <code>export LAUNCHPAD_ARGOCD_VERSION=\"v2.x.x\"</code>).</li> <li>Re-run the install so that manifests point to the new version:    - <code>phd_install_argo</code> (or <code>--argocd-only</code> / <code>--workflows-only</code>).</li> </ol> <p>Manifests are pulled from the configured install URLs; ensure the new version is available there.</p>"},{"location":"cluster/upgrade/#infrastructure-terraformopentofu","title":"Infrastructure (Terraform/OpenTofu)","text":"<p>To upgrade Harmony or other Terraform modules:</p> <ol> <li>Update the module <code>ref</code> or version in your cluster\u2019s Terraform code (e.g. in <code>main.tf</code> or <code>variables.tf</code>).</li> <li>Run <code>tofu plan</code> and review changes.</li> <li>Apply with <code>tofu apply</code>.</li> </ol> <p>Node pool or Kubernetes version changes may trigger rolling updates; check provider documentation for impact.</p>"},{"location":"cluster/upgrade/#related-documentation","title":"Related Documentation","text":"<ul> <li>Cluster Overview -  Cluster operations</li> <li>Infrastructure Provisioning -  Initial install and env vars</li> <li>Cluster Configuration -  Cluster-wide settings</li> <li>Backup -  Backup before upgrading</li> </ul>"},{"location":"cluster/upgrade/#see-also","title":"See Also","text":"<ul> <li>Infrastructure Overview -  Harmony and Terraform modules</li> <li>Instances Overview -  Instance upgrades</li> <li>User Guides Overview -  Task-based guides</li> </ul>"},{"location":"infrastructure/","title":"Overview","text":""},{"location":"infrastructure/#infrastructure-overview","title":"Infrastructure Overview","text":"<p>The Launchpad stack provides a solution for hosting Open edX instances on Kubernetes. This overview describes the core components and how they work together to deliver a scalable, maintainable platform.</p>"},{"location":"infrastructure/#kubernetes","title":"Kubernetes","text":"<p>Kubernetes serves as the foundation of the infrastructure, providing container orchestration and cluster management capabilities. The platform leverages Kubernetes for:</p> <ul> <li>Resource Management: Automated scheduling, scaling, and management of containerized applications</li> <li>Namespace Isolation: Each Open edX instance runs in its own namespace, ensuring proper resource isolation and security boundaries</li> <li>Configuration Management: ConfigMaps and Secrets for application configuration and sensitive data</li> <li>Persistent Storage: Volume management for file storage (for services like Redis)</li> </ul> <p>The infrastructure supports multiple cloud providers, including AWS (EKS) and DigitalOcean (DOKS), allowing organizations to choose the platform that best fits their requirements.</p> <p>Other providers can be used if the external dependencies (mainly Harmony) are supporting them.</p>"},{"location":"infrastructure/#argocd","title":"ArgoCD","text":"<p>ArgoCD provides GitOps-based continuous delivery for the platform. It monitors the cluster repository and automatically synchronizes the desired state defined in the instance's manifests with the actual state in the Kubernetes cluster.</p> <p>Key capabilities include:</p> <ul> <li>Declarative Configuration: Application definitions stored as code in the cluster repository per Open edX instance</li> <li>Automated Synchronization: Continuous monitoring and reconciliation of cluster state (if auto-sync is enabled)</li> <li>Rollback Capabilities: Easy reversion to previous application states</li> </ul> <p>ArgoCD applications are created automatically for each Open edX instance, enabling declarative deployment and management of the entire platform stack. This allows selective auto-deployment for instances as desired.</p>"},{"location":"infrastructure/#argo-workflows","title":"Argo Workflows","text":"<p>Argo Workflows orchestrates the provisioning and deprovisioning of users and infrastructure resources. It manages the lifecycle of databases, storage buckets, and other supporting services required by Open edX instances.</p> <p>The workflow system handles:</p> <ul> <li>Database Provisioning: Automated creation of MySQL and MongoDB databases with proper credentials and access controls</li> <li>Storage Setup: Provisioning of S3-compatible storage buckets for media files and static assets</li> <li>Resource Cleanup: Automated deprovisioning workflows that safely remove resources when instances are deleted</li> <li>Workflow Templates: Reusable workflow definitions for consistent resource management across instances</li> </ul>"},{"location":"infrastructure/#harmony","title":"Harmony","text":"<p>Harmony is Open edX's project that provides Terraform modules and Kubernetes configurations for deploying Open edX on Kubernetes clusters. It includes:</p> <ul> <li>Ingress Management: Nginx-based ingress controllers with automatic TLS certificate management via Let's Encrypt</li> <li>Monitoring Stack: Optional Prometheus and Grafana integration for metrics collection and visualization</li> <li>Backup Solutions: Velero integration for cluster backup and disaster recovery</li> <li>Resource Quotas: Configurable resource limits and quotas for cluster components</li> </ul> <p>The Harmony Terraform modules are integrated into the infrastructure provisioning process, ensuring consistent deployment of supporting services across different cloud providers.</p>"},{"location":"infrastructure/#tutor","title":"Tutor","text":"<p>Tutor is the official deployment tool for Open edX, providing a command-line interface for managing Open edX installations. In the Launchpad stack, Tutor is used to:</p> <ul> <li>Generate Kubernetes Manifests: Create Kubernetes YAML manifests to setup and configure services</li> <li>Manage Open edX Versions: Specify and build specific versions of the Open edX platform</li> <li>Plugin Management: Install and configure Tutor plugins that extend platform functionality</li> <li>Configuration Management: Centralized configuration through Tutor's environment-based settings</li> </ul> <p>Each Open edX instance uses Tutor to generate its Kubernetes deployment manifests, which are then managed through ArgoCD deployments.</p>"},{"location":"infrastructure/#picasso","title":"Picasso","text":"<p>Picasso is a GitHub Actions-based workflow system for building Docker images for Open edX services. It automates the image build process and integrates with the deployment pipeline.</p> <p>Picasso provides:</p> <ul> <li>Automated Image Building: GitHub Actions workflows that build Docker images for Open edX core services and Micro Frontends (MFEs)</li> <li>Image Tag Management: Automatic generation and updating of image tags in configuration files</li> <li>Multi-Service Support: Separate build workflows for different services (openedx, mfe, etc.)</li> <li>Registry Integration: Support for various container registries, including GitHub Container Registry (GHCR) that is used by this stack by default</li> </ul> <p>The build workflows are not triggered automatically when changes are made to instance configurations. The build must be triggered manually (or by automation scripts) using GitHub Actions.</p>"},{"location":"infrastructure/#drydock","title":"Drydock","text":"<p>Drydock is a Tutor plugin that enhances Open edX deployments on Kubernetes. It provides additional functionality and optimizations specifically designed for ArgoCD based deployments.</p> <p>Key features include:</p> <ul> <li>Init Jobs: Automated initialization tasks that run before the main application starts, such as database migrations and static asset collection</li> <li>Kubernetes Optimizations: Enhanced resource management and scheduling for Open edX workloads</li> <li>Registry Credentials: Secure handling of container registry authentication for private image pulls</li> <li>Plugin Integration: Seamless integration with other Tutor plugins in the Kubernetes deployment context</li> </ul> <p>Drydock is automatically installed and enabled for all instances, ensuring that Open edX deployments benefit from Kubernetes-specific optimizations and best practices.</p>"},{"location":"infrastructure/#component-integration","title":"Component Integration","text":"<p>These components work together to form a cohesive platform:</p> <ol> <li>Infrastructure Provisioning: Terraform modules (including Harmony) provision the Kubernetes cluster and supporting infrastructure</li> <li>Image Building: Picasso builds Docker images for Open edX services and updates configuration files</li> <li>Deployment: ArgoCD monitors Git repositories and deploys applications using manifests generated by Tutor</li> <li>Resource Management: Argo Workflows provisions databases and storage resources as needed</li> <li>Runtime: Drydock enhances the Kubernetes deployment with optimized configurations and initialization tasks</li> </ol> <p>This architecture provides a scalable, maintainable solution for hosting multiple Open edX instances with proper isolation, automated operations, and GitOps-based management. All based on maintained and open-source solutions.</p>"},{"location":"infrastructure/#related-documentation","title":"Related Documentation","text":"<ul> <li>Introduction -  Documentation home</li> <li>Provisioning -  Cluster and ArgoCD provisioning steps</li> <li>Cluster Repository Setup -  GitHub secrets, workflows, and ArgoCD repository connection</li> <li>Deprovisioning -  Removing infrastructure</li> <li>Custom Resources -  Workflow templates and ArgoCD resources</li> </ul>"},{"location":"infrastructure/#see-also","title":"See Also","text":"<ul> <li>Cluster Overview -  Cluster operations after provisioning</li> <li>Instances Overview -  Instance lifecycle</li> <li>Instance Provisioning -  Creating Open edX instances</li> </ul>"},{"location":"infrastructure/cluster-repository-setup/","title":"Cluster Repository Setup","text":""},{"location":"infrastructure/cluster-repository-setup/#cluster-repository-setup","title":"Cluster Repository Setup","text":"<p>After generating your cluster configuration with <code>phd_create_cluster</code> and deploying the infrastructure, you need to configure the cluster repository for GitHub Actions and ArgoCD. This guide walks through secrets, environment variables, workflows, and ArgoCD repository connection.</p>"},{"location":"infrastructure/cluster-repository-setup/#overview","title":"Overview","text":"<p>The cluster repository contains:</p> <ul> <li>Terraform infrastructure code</li> <li>Instance configurations (<code>instances/&lt;name&gt;/</code>)</li> <li>GitHub Actions workflows for instance lifecycle and image builds</li> </ul> <p>To use these workflows and allow ArgoCD to sync from the repository, you must:</p> <ol> <li>Push the repository to GitHub</li> <li>Configure GitHub Actions secrets</li> <li>Set up the ArgoCD project and connect the repository</li> </ol>"},{"location":"infrastructure/cluster-repository-setup/#pushing-the-repository-to-github","title":"Pushing the Repository to GitHub","text":"<p>The <code>phd_create_cluster</code> command initializes a git repository and adds a remote. Complete the setup:</p> <ol> <li> <p>Create an empty repository in your GitHub organization (e.g. <code>your-org/phd-production-cluster</code>).</p> </li> <li> <p>Push the cluster repository:</p> </li> </ol> <pre><code>cd phd-production-cluster\ngit push -u origin main\n</code></pre>"},{"location":"infrastructure/cluster-repository-setup/#configuring-github-actions-secrets","title":"Configuring GitHub Actions Secrets","text":"<p>Workflows read sensitive values from repository secrets. Configure them before running Create Instance, Build, or Delete Instance workflows.</p>"},{"location":"infrastructure/cluster-repository-setup/#where-to-add-secrets","title":"Where to Add Secrets","text":"<ol> <li>Open your cluster repository on GitHub.</li> <li>Go to Settings \u2192 Secrets and variables \u2192 Actions.</li> <li>Click New repository secret.</li> <li>Enter the Name and Value for each secret.</li> </ol>"},{"location":"infrastructure/cluster-repository-setup/#required-secrets","title":"Required Secrets","text":"Secret Required for Description <code>TERRAFORM_SECRETS</code> Create Instance, Delete Instance HCL content for <code>secrets.auto.tfvars</code> (see below) <code>PHD_DOCKER_REGISTRY_CREDENTIALS</code> Create Instance Base64-encoded <code>username:token</code> for pulling images <code>PHD_MYSQL_HOST</code> Create Instance, Delete Instance MySQL server hostname <code>PHD_MYSQL_PORT</code> Create Instance, Delete Instance MySQL port (default: <code>3306</code>) <code>PHD_MYSQL_ROOT_USER</code> Create Instance, Delete Instance MySQL admin username <code>PHD_MYSQL_ROOT_PASSWORD</code> Create Instance, Delete Instance MySQL admin password <code>PHD_MONGODB_HOST</code> Create Instance, Delete Instance MongoDB hostname (for direct connection) <code>PHD_MONGODB_PORT</code> Create Instance, Delete Instance MongoDB port (default: <code>27017</code>) <code>PHD_MONGODB_PROVIDER</code> Create Instance, Delete Instance <code>digitalocean_api</code> or <code>atlas</code> <code>PHD_MONGODB_CLUSTER_ID</code> Create Instance, Delete Instance DigitalOcean MongoDB cluster ID (or Atlas project ID) <code>PHD_MONGODB_REPLICA_SET</code> Create Instance, Delete Instance MongoDB replica set name <code>PHD_MONGODB_AUTH_SOURCE</code> Create Instance, Delete Instance MongoDB auth source (default: <code>admin</code>) <code>PHD_DIGITALOCEAN_TOKEN</code> Create Instance, Delete Instance DigitalOcean API token <code>PHD_STORAGE_TYPE</code> Create Instance, Delete Instance <code>s3</code> or <code>spaces</code> <code>PHD_STORAGE_REGION</code> Create Instance, Delete Instance Region (e.g. <code>us-east-1</code>, <code>nyc3</code>) <code>PHD_STORAGE_ACCESS_KEY_ID</code> Create Instance, Delete Instance S3/Spaces access key ID <code>PHD_STORAGE_SECRET_ACCESS_KEY</code> Create Instance, Delete Instance S3/Spaces secret access key <code>SSH_PRIVATE_KEY</code> Create Instance, Build, Build All, Delete Instance, Update Instance Private SSH key for cloning the cluster repo and private dependencies"},{"location":"infrastructure/cluster-repository-setup/#terraform_secrets-format","title":"TERRAFORM_SECRETS Format","text":"<p><code>TERRAFORM_SECRETS</code> is the exact content that would go in <code>infrastructure/secrets.auto.tfvars</code>. The workflow writes it to that file for Terraform/OpenTofu.</p> <p>DigitalOcean:</p> <pre><code>access_token     = \"dop_v1_your_digitalocean_token\"\naccess_key_id    = \"your_spaces_access_key_id\"\nsecret_access_key = \"your_spaces_secret_access_key\"\n</code></pre> <p>AWS:</p> <pre><code>aws_access_key_id     = \"AKIA...\"\naws_secret_access_key = \"your_aws_secret_key\"\n</code></pre> <p>Create the secret by copying the full HCL block (including variable names) and pasting it as the secret value. Do not wrap it in quotes or encode it further.</p>"},{"location":"infrastructure/cluster-repository-setup/#generating-common-secrets","title":"Generating Common Secrets","text":"<p>Docker registry credentials (for <code>PHD_DOCKER_REGISTRY_CREDENTIALS</code>):</p> <pre><code># GitHub Container Registry\necho -n \"github_username:ghp_your_personal_access_token\" | base64\n\n# Docker Hub\necho -n \"dockerhub_username:dockerhub_token\" | base64\n</code></pre> <p>Use a GitHub PAT with <code>read:packages</code> for GHCR.</p> <p>SSH private key (for <code>SSH_PRIVATE_KEY</code>):</p> <pre><code># Generate a new deploy key (if you don't have one)\nssh-keygen -t ed25519 -C \"github-actions-deploy\" -f deploy_key -N \"\"\n\n# Add deploy_key.pub as a Deploy Key in your repo (Settings \u2192 Deploy keys)\n# Paste the contents of deploy_key (private key) as the SSH_PRIVATE_KEY secret\ncat deploy_key\n</code></pre> <p>The same key is used for cloning the cluster repository and any private dependencies (e.g. edx-platform forks, Tutor plugins).</p>"},{"location":"infrastructure/cluster-repository-setup/#github-actions-workflows","title":"GitHub Actions Workflows","text":"<p>The cluster repository includes reusable workflows that you trigger manually.</p>"},{"location":"infrastructure/cluster-repository-setup/#workflow-overview","title":"Workflow Overview","text":"Workflow Trigger Purpose Create Instance Manual (workflow_dispatch) Creates a new instance: config, provision workflows, ArgoCD Application Build Image Manual Builds a single service image (openedx or mfe) for an instance Build All Images Manual Builds both openedx and mfe images for an instance Delete Instance Manual Removes an instance and runs deprovision workflows Update Instance Manual Merges JSON config into an instance's <code>config.yml</code> pre-commit On push/PR Runs linting and formatting"},{"location":"infrastructure/cluster-repository-setup/#how-to-trigger-workflows","title":"How to Trigger Workflows","text":"<ol> <li>Open your cluster repository on GitHub.</li> <li>Go to the Actions tab.</li> <li>Select the workflow from the left sidebar (e.g. \"Create Instance\").</li> <li>Click Run workflow.</li> <li>Fill in the inputs and run.</li> </ol>"},{"location":"infrastructure/cluster-repository-setup/#common-workflow-inputs","title":"Common Workflow Inputs","text":"Input Workflows Description INSTANCE_NAME Create, Build, Build All, Delete, Update Instance identifier (DNS-compliant, e.g. <code>my-instance</code>) STRAIN_REPOSITORY_BRANCH Build, Build All Branch to use for the strain (default: <code>main</code>). The strain is the cluster repo; this is the branch containing <code>instances/&lt;name&gt;/</code> SERVICE Build Service to build: <code>openedx</code> or <code>mfe</code> PHD_CLI_VERSION Create, Build, Build All, Delete Git ref (branch/tag/SHA) of phd-cluster-template for the CLI (default: <code>main</code>) RUNNER_WORKFLOW_LABEL All GitHub Actions runner label (default: <code>ubuntu-latest</code>). Use <code>self-hosted</code> for self-hosted runners PICASSO_VERSION Build, Build All Git ref of Picasso for image builds PHD_OPENCRAFT_MANIFESTS_VERSION Delete Git ref for OpenCraft manifests (default: <code>main</code>) EDX_PLATFORM_VERSION Create edX Platform branch/tag (default: <code>release/teak.3</code>) TUTOR_VERSION Create Tutor version (default from cluster template) INSTANCE_TEMPLATE_VERSION Create Instance template version (default: <code>main</code>) CONFIG Update JSON object to merge into instance config (e.g. <code>{\"KEY\": \"value\"}</code>)"},{"location":"infrastructure/cluster-repository-setup/#strain-and-repository","title":"Strain and Repository","text":"<p>The strain is the cluster repository itself. The branch (<code>STRAIN_REPOSITORY_BRANCH</code>) determines which branch of the cluster repo is used when building images. For instance <code>my-instance</code>, the build uses <code>instances/my-instance/</code> from that branch.</p>"},{"location":"infrastructure/cluster-repository-setup/#argocd-project-and-repository-connection","title":"ArgoCD Project and Repository Connection","text":"<p>ArgoCD must be able to clone the cluster repository to sync applications. Configure it after installing ArgoCD and Argo Workflows.</p>"},{"location":"infrastructure/cluster-repository-setup/#step-1-create-or-use-the-default-project","title":"Step 1: Create or Use the Default Project","text":"<p>ArgoCD applications use a project (e.g. <code>phd-production</code>). The default project may already exist. To create one:</p> <ol> <li>Log into the ArgoCD UI.</li> <li>Go to Settings \u2192 Projects.</li> <li>Create a project or use the default.</li> </ol>"},{"location":"infrastructure/cluster-repository-setup/#step-2-connect-the-repository","title":"Step 2: Connect the Repository","text":"<ol> <li>In ArgoCD, go to Settings \u2192 Repositories.</li> <li>Click Connect Repo.</li> <li>Choose the connection method:</li> </ol>"},{"location":"infrastructure/cluster-repository-setup/#github-ssh-deploy-key","title":"GitHub (SSH Deploy Key)","text":"<ol> <li>Connection method: Via SSH</li> <li>Repository URL: <code>git@github.com:your-org/phd-production-cluster.git</code></li> <li>SSH private key: Paste the private key (same content as <code>SSH_PRIVATE_KEY</code> if you use it for GitHub Actions)</li> </ol> <p>To add the SSH key to GitHub as a Deploy Key:</p> <ul> <li>Go to your cluster repo \u2192 Settings \u2192 Deploy keys.</li> <li>Add the public key (<code>.pub</code> file).</li> </ul> <p>For read-only access, you can create a deploy key with only clone permission. For write access (e.g. if ArgoCD writes back), enable write access.</p>"},{"location":"infrastructure/cluster-repository-setup/#github-https-with-personal-access-token","title":"GitHub (HTTPS with Personal Access Token)","text":"<ol> <li>Connection method: Via HTTPS</li> <li>Repository URL: <code>https://github.com/your-org/phd-production-cluster.git</code></li> <li>Username: Your GitHub username (or <code>x-access-token</code> for fine-grained PATs)</li> <li>Password: GitHub Personal Access Token with <code>repo</code> scope</li> </ol>"},{"location":"infrastructure/cluster-repository-setup/#step-3-verify-connection","title":"Step 3: Verify Connection","text":"<p>After adding the repository, ArgoCD should show it as connected. Applications that reference this repo will be able to sync.</p>"},{"location":"infrastructure/cluster-repository-setup/#step-4-project-configuration","title":"Step 4: Project Configuration","text":"<p>Ensure the project allows the repository:</p> <ol> <li>Go to Settings \u2192 Projects \u2192 your project.</li> <li>Under Source Repositories, add your cluster repository URL.</li> <li>Under Destinations, add the cluster (e.g. <code>https://kubernetes.default.svc</code>).</li> </ol>"},{"location":"infrastructure/cluster-repository-setup/#related-documentation","title":"Related Documentation","text":"<ul> <li>Infrastructure Provisioning - Cluster creation and ArgoCD installation</li> <li>Instance Provisioning - Creating instances</li> <li>Instance Configuration - config.yml and manifests</li> <li>Instance Docker Images - Building images with Picasso</li> </ul>"},{"location":"infrastructure/custom-resources/","title":"Custom Resources","text":""},{"location":"infrastructure/custom-resources/#custom-resources","title":"Custom Resources","text":"<p>Harmony provides the core cluster resources by default, but you often need extra infrastructure or applications, for example, PR sandbox automation or a dedicated S3 bucket for a plugin. The cluster repository supports adding these as custom Terraform and ArgoCD resources.</p>"},{"location":"infrastructure/custom-resources/#infrastructure-resources","title":"Infrastructure Resources","text":"<p>The <code>infrastructure</code> directory in the cluster repository is standard Terraform: Harmony\u2019s modules are used to set up the cluster, and the rest is regular Terraform code. You can define any additional Terraform resources required for your operation in this directory (e.g. extra buckets, IAM, or other cloud resources).</p>"},{"location":"infrastructure/custom-resources/#argocd-resources","title":"ArgoCD Resources","text":"<p>Once the cluster is set up, ArgoCD has access to the cluster repository over SSH. You can register any ArgoCD Application, not only Open edX instances. That allows you to deploy other services, such as PR sandbox automation, while keeping everything in code and under version control.</p> <p>ArgoCD can load configuration from any directory in the repo. For clarity and consistency, use a dedicated <code>manifests</code> directory for non\u2013Open-edX applications and shared manifests.</p>"},{"location":"infrastructure/custom-resources/#related-documentation","title":"Related Documentation","text":"<ul> <li>Infrastructure Overview -  Core components (Argo Workflows, ArgoCD)</li> <li>Infrastructure Provisioning -  How provision workflows and templates are applied</li> <li>Infrastructure Deprovisioning -  Deprovision workflows and cleanup</li> </ul>"},{"location":"infrastructure/custom-resources/#see-also","title":"See Also","text":"<ul> <li>Instance Provisioning -  Instance-level provisioning</li> <li>Instance Deprovisioning -  Instance resource cleanup</li> <li>Instance Configuration -  Instance manifests and config</li> </ul>"},{"location":"infrastructure/deprovisioning/","title":"Deprovisioning","text":""},{"location":"infrastructure/deprovisioning/#infrastructure-deprovisioning","title":"Infrastructure Deprovisioning","text":"<p>Deprovisioning is removing the Kubernetes cluster and all associated resources when they are no longer needed. This includes uninstalling GitOps tools (ArgoCD and Argo Workflows), destroying infrastructure resources, and cleaning up configuration files.</p> <p>The deprovisioning process should be performed in reverse order of provisioning:</p> <ol> <li>Remove All Instances: Delete all Open edX instances first</li> <li>Uninstall GitOps Tools: Remove ArgoCD and Argo Workflows</li> <li>Destroy Infrastructure: Destroy Kubernetes cluster and supporting resources using Terraform/OpenTofu</li> <li>Clean Up Configuration: Remove local cluster configuration files</li> </ol> <p>Warning: Deprovisioning is a destructive operation that permanently removes all resources and data. Ensure you have backups of any important data before proceeding.</p>"},{"location":"infrastructure/deprovisioning/#prerequisites","title":"Prerequisites","text":"<p>Before deprovisioning infrastructure, ensure:</p> <ul> <li>All Instances Removed: All Open edX instances have been deleted</li> <li>Backups Verified: Any required backups have been created and verified</li> <li>Access Credentials: Valid credentials for cloud provider and Kubernetes cluster</li> <li>Terraform/OpenTofu State: Access to Terraform state file (for infrastructure destruction)</li> </ul>"},{"location":"infrastructure/deprovisioning/#deprovisioning-steps","title":"Deprovisioning Steps","text":""},{"location":"infrastructure/deprovisioning/#step-1-verify-no-active-instances","title":"Step 1: Verify No Active Instances","text":"<p>Before deprovisioning infrastructure, ensure all Open edX instances have been removed:</p> <pre><code># List all namespaces (excluding system namespaces)\nkubectl get namespaces --field-selector metadata.name!=kube-system,metadata.name!=kube-public,metadata.name!=kube-node-lease\n\n# Check for ArgoCD applications\nkubectl get applications -n argocd\n\n# Verify no instance namespaces exist\nkubectl get namespaces | grep -v \"kube-\\|argocd\\|argo\\|default\"\n</code></pre> <p>If any instances remain, delete them first using <code>phd_delete_instance</code>, the GitHub Actions workflow, or manually.</p>"},{"location":"infrastructure/deprovisioning/#step-2-uninstall-argocd-and-argo-workflows","title":"Step 2: Uninstall ArgoCD and Argo Workflows","text":"<p>Remove the GitOps tools from the cluster. While there's no dedicated uninstall command, you can remove them manually:</p> <p>Uninstall ArgoCD:</p> <pre><code># Delete ArgoCD namespace (this removes all ArgoCD resources)\nkubectl delete namespace argocd\n\n# Wait for namespace to be fully deleted\nkubectl wait --for=delete namespace/argocd --timeout=300s\n</code></pre> <p>Uninstall Argo Workflows:</p> <pre><code># Delete workflow templates first\nkubectl delete clusterworkflowtemplates --all\n\n# Delete Argo Workflows namespace\nkubectl delete namespace argo\n\n# Wait for namespace to be fully deleted\nkubectl wait --for=delete namespace/argo --timeout=300s\n</code></pre> <p>Verify Removal:</p> <pre><code># Verify namespaces are gone\nkubectl get namespace argocd  # Should return \"not found\"\nkubectl get namespace argo    # Should return \"not found\"\n\n# Verify no ArgoCD resources remain\nkubectl get all -n argocd  # Should return \"not found\"\n\n# Verify no Argo Workflows resources remain\nkubectl get all -n argo  # Should return \"not found\"\n</code></pre> <p>Note: If namespaces are stuck in \"Terminating\" state, see the Troubleshooting section.</p>"},{"location":"infrastructure/deprovisioning/#step-3-clean-up-remaining-resources","title":"Step 3: Clean Up Remaining Resources","text":"<p>Remove any remaining resources that might prevent infrastructure destruction:</p> <p>We are listing the PVs and PVCs in order to keep a record of what was used by the cluster. In case the cluster deletion goes sideways, we have easier job identifying dangling resources.</p> <pre><code># List persistent volumes\nkubectl get pv\n\n# List persistent volume claims\nkubectl get pvc --all-namespaces\n\n# List storage classes\nkubectl get storageclass\n\n# List custom resource definitions\nkubectl get crd\n</code></pre> <p>Delete selective resources (replace <code>&lt;name&gt;</code> and <code>&lt;namespace&gt;</code> with actual values):</p> <pre><code># Delete a persistent volume claim (typically required before deleting its bound PV)\nkubectl delete pvc &lt;pvc-name&gt; -n &lt;namespace&gt;\n\n# Delete a persistent volume\nkubectl delete pv &lt;pv-name&gt;\n\n# Delete a storage class (only if not in use)\nkubectl delete storageclass &lt;storageclass-name&gt;\n\n# Delete a custom resource definition (removes the CRD and all instances of that resource)\nkubectl delete crd &lt;crd-name&gt;\n</code></pre> <p>Remove Custom Resources (if needed):</p> <pre><code># Remove ArgoCD CRDs (if not removed with namespace)\nkubectl delete crd applications.argoproj.io\nkubectl delete crd application sets.argoproj.io\nkubectl delete crd appprojects.argoproj.io\n\n# Remove Argo Workflows CRDs (if not removed with namespace)\nkubectl delete crd clusterworkflowtemplates.argoproj.io\nkubectl delete crd cronworkflows.argoproj.io\nkubectl delete crd workflows.argoproj.io\nkubectl delete crd workflowtemplates.argoproj.io\nkubectl delete crd workfloweventbindings.argoproj.io\n</code></pre>"},{"location":"infrastructure/deprovisioning/#step-4-destroy-infrastructure","title":"Step 4: Destroy Infrastructure","text":"<p>Destroy the infrastructure using Terraform or OpenTofu. This will remove the Kubernetes cluster and all associated cloud resources.</p> <p>Navigate to Infrastructure Directory:</p> <pre><code>cd phd-production-cluster/infrastructure-aws  # or infrastructure-digitalocean\n</code></pre> <p>Configure Backend Credentials:</p> <p>See the provisioning guide on how to setup the backend credentials.</p> <p>Review Destruction Plan:</p> <pre><code># Review what will be destroyed\ntofu plan -destroy\n</code></pre> <p>Destroy Infrastructure:</p> <pre><code># Destroy all infrastructure\ntofu destroy\n</code></pre> <p>What Gets Destroyed:</p> <ul> <li>Kubernetes Cluster: EKS or DOKS cluster</li> <li>Node Groups: All worker nodes</li> <li>Databases: Managed MySQL and MongoDB instances</li> <li>Storage: S3 buckets or DigitalOcean Spaces</li> <li>Networking: Load balancers, VPC resources (depending on configuration)</li> <li>Other Resources: Any other resources created by Terraform modules</li> </ul> <p>Important Notes:</p> <ul> <li>Some resources may take time to destroy (especially databases and storage)</li> <li>External databases and storage will be destroyed</li> <li>Review the destruction plan carefully before confirming</li> </ul>"},{"location":"infrastructure/deprovisioning/#step-5-clean-up-local-configuration","title":"Step 5: Clean Up Local Configuration","text":"<p>After infrastructure is destroyed, you can optionally remove local cluster configuration:</p> <p>Remove Cluster Directory:</p> <pre><code># Navigate to parent directory\ncd ../..\n\n# Remove cluster configuration directory\nrm -rf phd-production-cluster\n</code></pre> <p>Remove kubeconfig:</p> <pre><code># Remove cluster-specific kubeconfig\nrm ~/.kube/config-cluster\n\n# Or remove from main kubeconfig if merged\nkubectl config delete-context &lt;cluster-context-name&gt;\nkubectl config delete-cluster &lt;cluster-name&gt;\n</code></pre> <p>Note: You may want to keep the cluster configuration directory for reference or to recreate the cluster later.</p>"},{"location":"infrastructure/deprovisioning/#troubleshooting","title":"Troubleshooting","text":""},{"location":"infrastructure/deprovisioning/#namespace-stuck-in-terminating-state","title":"Namespace Stuck in Terminating State","text":"<p>If ArgoCD or Argo Workflows namespaces are stuck in \"Terminating\" state:</p> <p>Check for Blocking Resources:</p> <pre><code># List all resources in the namespace\nkubectl api-resources --verbs=list --namespaced -o name | \\\n  xargs -n 1 kubectl get --show-kind --ignore-not-found -n argocd\n\n# Check for finalizers\nkubectl get namespace argocd -o yaml | grep finalizers\n</code></pre> <p>Force Remove Finalizers:</p> <pre><code># Edit namespace to remove finalizers\nkubectl patch namespace argocd \\\n  -p '{\"metadata\":{\"finalizers\":[]}}' \\\n  --type=merge\n</code></pre> <p>Force Delete Resources:</p> <p>If specific resources are blocking deletion:</p> <pre><code># Find resources with finalizers\nkubectl get all -n argocd -o yaml | grep -A 5 finalizers\n\n# Remove finalizers from specific resource\nkubectl patch &lt;resource-type&gt; &lt;resource-name&gt; -n argocd \\\n  -p '{\"metadata\":{\"finalizers\":[]}}' \\\n  --type=merge\n</code></pre>"},{"location":"infrastructure/deprovisioning/#infrastructure-destruction-issues","title":"Infrastructure Destruction Issues","text":"<p>Terraform/OpenTofu Errors:</p> <ul> <li>State Lock: If state is locked, check for other running Terraform processes</li> <li>Resource Dependencies: Some resources may have dependencies preventing deletion</li> <li>Provider Timeouts: Large resources may take longer than default timeouts</li> </ul> <p>Common Solutions:</p> <pre><code># Force unlock state (use with caution)\ntofu force-unlock &lt;lock-id&gt;\n\n# Destroy specific resources first\ntofu destroy -target=&lt;resource-address&gt;\n\n# Increase timeout for slow resources\nexport TF_CLI_ARGS=\"-timeout=30m\"\ntofu destroy\n</code></pre> <p>Resources Not Destroyed:</p> <p>Some resources may not be destroyed if:</p> <ul> <li>They're managed outside of Terraform</li> <li>They have deletion protection enabled</li> <li>They're shared resources used by other clusters</li> </ul> <p>Manually remove these resources through the cloud provider console if needed.</p>"},{"location":"infrastructure/deprovisioning/#kubernetes-cluster-access-issues","title":"Kubernetes Cluster Access Issues","text":"<p>Cannot Access Cluster During Destruction:</p> <ul> <li>If the cluster API server is already destroyed, you cannot use <code>kubectl</code> commands</li> <li>Some resources may be destroyed automatically by the cloud provider</li> <li>Check cloud provider console for remaining resources</li> </ul> <p>Orphaned Resources:</p> <p>After cluster destruction, some resources may remain:</p> <ul> <li>Load balancers</li> <li>Persistent volumes (if not properly cleaned up)</li> <li>Security groups</li> <li>IAM roles and policies</li> </ul> <p>Manually clean these up through the cloud provider console.</p>"},{"location":"infrastructure/deprovisioning/#state-file-issues","title":"State File Issues","text":"<p>State File Not Found:</p> <ul> <li>Verify backend configuration is correct</li> <li>Check that state file exists in the backend storage</li> <li>Ensure backend credentials have read access</li> </ul> <p>State File Corruption:</p> <pre><code># Backup state file first\ntofu state pull &gt; state-backup.json\n\n# Try to refresh state\ntofu refresh\n\n# If refresh fails, you may need to import resources or recreate state\n</code></pre>"},{"location":"infrastructure/deprovisioning/#partial-destruction","title":"Partial Destruction","text":"<p>If destruction partially succeeds:</p> <ol> <li>Review Remaining Resources: Check what resources still exist</li> <li>Manual Cleanup: Remove remaining resources through cloud provider console</li> <li>Update State: Use <code>tofu state rm</code> to remove destroyed resources from state</li> <li>Retry Destruction: Run <code>tofu destroy</code> again to clean up remaining resources</li> </ol>"},{"location":"infrastructure/deprovisioning/#data-recovery","title":"Data Recovery","text":"<p>If you need to recover data after deprovisioning:</p> <ul> <li>Backups: Check if Velero or other backup solutions created backups</li> <li>Database Snapshots: Some cloud providers maintain database snapshots</li> <li>Storage Buckets: If buckets weren't destroyed, data may still be accessible</li> <li>Volume Snapshots: Check for volume snapshots in your cloud provider</li> </ul> <p>Note: Recovery may not be possible if backups weren't configured or have been deleted.</p>"},{"location":"infrastructure/deprovisioning/#best-practices","title":"Best Practices","text":""},{"location":"infrastructure/deprovisioning/#before-deprovisioning","title":"Before Deprovisioning","text":"<ol> <li>Create Backups: Ensure all important data is backed up</li> <li>Document Configuration: Save cluster configuration for future reference</li> <li>Verify Dependencies: Check that no other systems depend on this infrastructure</li> </ol>"},{"location":"infrastructure/deprovisioning/#during-deprovisioning","title":"During Deprovisioning","text":"<ol> <li>Follow Order: Remove instances \u2192 Uninstall tools \u2192 Destroy infrastructure</li> <li>Monitor Progress: Watch for errors and address them promptly</li> <li>Verify Removal: Confirm resources are actually destroyed, not just marked for deletion</li> <li>Keep Logs: Save logs of the deprovisioning process for troubleshooting</li> </ol>"},{"location":"infrastructure/deprovisioning/#after-deprovisioning","title":"After Deprovisioning","text":"<ol> <li>Verify Cleanup: Check cloud provider console for any remaining resources</li> <li>Update Documentation: Note that infrastructure has been deprovisioned</li> <li>Archive Configuration: Keep cluster configuration in version control for reference</li> <li>Review Costs: Verify that cloud provider billing reflects the destroyed resources</li> </ol>"},{"location":"infrastructure/deprovisioning/#next-steps","title":"Next Steps","text":"<p>After successfully deprovisioning infrastructure:</p> <ol> <li>Verify Billing: Confirm cloud provider billing reflects destroyed resources</li> <li>Archive Configuration: Keep cluster configuration for future reference</li> <li>Update Documentation: Document that the cluster has been deprovisioned</li> <li>Clean Up Credentials: Rotate or remove any credentials that were used for this cluster</li> </ol>"},{"location":"infrastructure/deprovisioning/#related-documentation","title":"Related Documentation","text":"<ul> <li>Infrastructure Overview -  Core components and architecture</li> <li>Provisioning -  How infrastructure is provisioned</li> <li>Cluster Overview -  Cluster operations</li> <li>Instance Deprovisioning -  Deleting Open edX instances</li> </ul>"},{"location":"infrastructure/deprovisioning/#see-also","title":"See Also","text":"<ul> <li>Cluster Backup -  Backup before deprovisioning</li> <li>Cluster Restore -  Restore procedures</li> <li>Custom Resources -  Resources removed during deprovisioning</li> </ul>"},{"location":"infrastructure/provisioning/","title":"Provisioning","text":""},{"location":"infrastructure/provisioning/#infrastructure-provisioning","title":"Infrastructure Provisioning","text":"<p>Infrastructure provisioning sets up a complete Kubernetes cluster environment for hosting Open edX instances. This includes creating the cluster configuration, deploying the underlying infrastructure (Kubernetes cluster, databases, storage), and installing ArgoCD and Argo Workflows.</p> <p>The provisioning process consists of three main phases:</p> <ol> <li>Cluster Configuration: Generate cluster configuration files using cookiecutter templates</li> <li>Infrastructure Deployment: Deploy Kubernetes cluster and supporting infrastructure using Terraform/OpenTofu</li> <li>GitOps Tools Installation: Install and configure ArgoCD and Argo Workflows for automated deployments</li> </ol>"},{"location":"infrastructure/provisioning/#prerequisites","title":"Prerequisites","text":"<p>Before provisioning infrastructure, ensure you have:</p> <ul> <li>Launchpad CLI: Installed and configured (see Quick Start)</li> <li>Cookiecutter: Installed for cluster template generation (<code>pip install cookiecutter</code>)</li> <li>Cloud Provider Access: Valid credentials for AWS or DigitalOcean</li> <li>Terraform/OpenTofu: Installed (OpenTofu is recommended)</li> <li>kubectl: Installed</li> <li>Git Repository: Access to a GitHub organization with repository create access (will create the repository during provisioning)</li> </ul>"},{"location":"infrastructure/provisioning/#provisioning-steps","title":"Provisioning Steps","text":""},{"location":"infrastructure/provisioning/#step-1-create-cluster-configuration","title":"Step 1: Create Cluster Configuration","text":"<p>The first step is to generate the cluster configuration using the <code>phd_create_cluster</code> command. This creates a directory structure with Terraform modules, GitHub Actions workflows, and configuration files.</p> <p>Install the CLI (if not already installed):</p> <pre><code># Install uv (if needed)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install PHD CLI as a tool (persistent)\nuv tool install git+https://github.com/open-craft/phd-cluster-template.git#subdirectory=tooling\n\n# Or run without installing (one-off)\nuvx --from git+https://github.com/open-craft/phd-cluster-template.git#subdirectory=tooling phd_create_cluster --help\n</code></pre> <p>Create cluster configuration:</p> <pre><code># Create cluster with custom options\nphd_create_cluster \"Launchpad Production Cluster\" \"cluster.domain\" \\\n  --environment production \\\n  --cloud-provider aws \\\n  --cloud-region us-east-1 \\\n  --github-organization your-org \\\n  --output-dir ./clusters\n</code></pre> <p>Command Options:</p> <ul> <li><code>cluster_name</code>: Display name for the cluster</li> <li><code>cluster_domain</code>: Domain name for the cluster (e.g., <code>cluster.domain</code>)</li> <li><code>--environment</code>: Environment name (default: <code>production</code>)</li> <li><code>--cloud-provider</code>: Cloud provider - <code>aws</code> or <code>digitalocean</code></li> <li><code>--cloud-region</code>: Region for the cloud provider</li> <li><code>--harmony-module-version</code>: Harmony Terraform module version/commit hash</li> <li><code>--opencraft-module-version</code>: OpenCraft Terraform module version</li> <li><code>--picasso-version</code>: Picasso version for image building</li> <li><code>--tutor-version</code>: Tutor version to use</li> <li><code>--github-organization</code>: GitHub organization name</li> <li><code>--github-repository</code>: Custom GitHub repository URL (auto-generated if not provided)</li> <li><code>--output-dir</code>: Directory where cluster configuration will be created</li> </ul> <p>What This Creates:</p> <ul> <li>Cluster directory with normalized name (e.g., <code>phd-production-cluster</code>)</li> <li>Terraform infrastructure modules for AWS or DigitalOcean</li> <li>GitHub Actions workflows for building images and managing instances</li> <li>Instance template directory structure</li> <li>Configuration files and documentation</li> </ul> <p>After pushing the repository to GitHub, configure GitHub Actions secrets and ArgoCD before creating instances.</p>"},{"location":"infrastructure/provisioning/#step-2-deploy-infrastructure","title":"Step 2: Deploy Infrastructure","text":"<p>After generating the cluster configuration, deploy the infrastructure using Terraform or OpenTofu.</p> <p>Navigate to Infrastructure Directory:</p> <pre><code>cd phd-production-cluster/infrastructure\n</code></pre> <p>Configure Backend Credentials:</p> <p>Backend credentials must be provided via environment variables or <code>backend.hcl</code> (recommended) to avoid storing them in state files:</p> <pre><code>bucket     = \"tfstate-bucket-name\"\nkey        = \"terraform.tfstate\"\naccess_key = \"access-key\"\nsecret_key = \"secret-key\"\n</code></pre> <p>Initialize and Deploy:</p> <pre><code># Initialize Terraform/OpenTofu\ntofu init -backend-config=backend.hcl\n\n# Review the deployment plan\ntofu plan\n\n# Apply the infrastructure\ntofu apply\n</code></pre> <p>What Gets Deployed:</p> <ul> <li>Kubernetes Cluster: EKS (AWS) or DOKS (DigitalOcean)</li> <li>Databases: Managed MySQL and MongoDB instances</li> <li>Storage: S3 buckets or DigitalOcean Spaces</li> <li>Networking: VPC with private/public subnets, load balancers</li> <li>Harmony Components: Ingress controllers, monitoring (optional), backups (optional)</li> </ul> <p>Important: After deployment completes, note the <code>.kubeconfig</code>will be created. You'll need this to access the cluster.</p>"},{"location":"infrastructure/provisioning/#step-3-configure-kubectl","title":"Step 3: Configure kubectl","text":"<p>Configure <code>kubectl</code> to access your newly created cluster:</p> <pre><code># Activate the configuration\nsource ../activate\n\n# Verify cluster access\nkubectl get nodes\n</code></pre>"},{"location":"infrastructure/provisioning/#step-4-install-argocd-and-argo-workflows","title":"Step 4: Install ArgoCD and Argo Workflows","text":"<p>Install the GitOps tools required for managing Open edX instances.</p> <p>Set Required Environment Variable:</p> <pre><code>export LAUNCHPAD_CLUSTER_DOMAIN=\"cluster.domain\"\nexport LAUNCHPAD_DOCKER_REGISTRY_CREDENTIALS=\"base64 encoded docker registry credentials\"\n</code></pre> <p>Install Both Tools:</p> <pre><code># Install both ArgoCD and Argo Workflows\nphd_install_argo\n</code></pre> <p>Or, Install Selectively:</p> <pre><code># Install only ArgoCD\nphd_install_argo --argocd-only\n\n# Install only Argo Workflows\nphd_install_argo --workflows-only\n</code></pre> <p>What Gets Installed:</p> <p>ArgoCD: - ArgoCD namespace and core components - Ingress configuration for web UI access - Admin password (auto-generated if not provided via <code>LAUNCHPAD_ARGO_ADMIN_PASSWORD</code>) - Base configuration - Docker registry pull credentials</p> <p>Argo Workflows: - Argo Workflows namespace and core components - Ingress configuration for workflow UI - Admin authentication setup - Workflow executor service account and token - Provisioning/deprovisioning workflow templates (MySQL, MongoDB, Storage) - Docker registry pull credentials</p> <p>Access the UIs:</p> <p>After installation, you can access:</p> <ul> <li>ArgoCD: <code>https://argocd.cluster.domain</code></li> <li>Argo Workflows: <code>http://localhost:2746</code> (after <code>kubectl port-forward svc/argo-server 2746:2746 -n argo</code>)</li> </ul> <p>Use the admin password (displayed during installation or set via <code>LAUNCHPAD_ARGO_ADMIN_PASSWORD</code>) to log in.</p>"},{"location":"infrastructure/provisioning/#step-5-configure-argocd-projects-and-repository","title":"Step 5: Configure ArgoCD Projects and Repository","text":"<p>ArgoCD must be able to clone the cluster repository to sync applications. Configure the repository connection and project before creating instances.</p> <p>See Cluster Repository Setup for detailed instructions on:</p> <ul> <li>Connecting the repository via SSH (GitHub deploy key)</li> <li>Configuring the ArgoCD project</li> <li>Verifying the connection</li> </ul>"},{"location":"infrastructure/provisioning/#step-6-verify-installation","title":"Step 6: Verify Installation","text":"<p>Verify that all components are installed and functioning:</p> <pre><code># Check ArgoCD pods\nkubectl get pods -n argocd\n\n# Check Argo Workflows pods\nkubectl get pods -n argo\n\n# Check workflow templates\nkubectl get clusterworkflowtemplates\n\n# Verify ingress\nkubectl get ingress -n argocd\nkubectl get ingress -n argo\n</code></pre>"},{"location":"infrastructure/provisioning/#creating-argocd-users","title":"Creating ArgoCD Users","text":"<p>After installing ArgoCD and Argo Workflows, you can create ArgoCD users using the <code>phd_create_argo_user</code> command.</p>"},{"location":"infrastructure/provisioning/#basic-usage","title":"Basic Usage","text":"<p>Create a user with default role (developer):</p> <pre><code># Set required environment variable\nexport LAUNCHPAD_CLUSTER_DOMAIN=\"cluster.domain\"\n\n# Create user (will prompt for password)\nphd_create_argo_user john.doe\n</code></pre> <p>Create a user with specific role and password:</p> <pre><code># Create admin user\nphd_create_argo_user admin.user \\\n  --role admin \\\n  --password \"secure-password\"\n\n# Create developer user\nphd_create_argo_user developer.user \\\n  --role developer \\\n  --password \"secure-password\"\n\n# Create readonly user\nphd_create_argo_user viewer.user \\\n  --role readonly \\\n  --password \"secure-password\"\n</code></pre>"},{"location":"infrastructure/provisioning/#user-roles","title":"User Roles","text":"<p>The system supports three user roles with different ArgoCD permission levels:</p> <p>Admin Role: - Full access to all applications and projects - Create, read, update, delete all applications and projects</p> <p>Developer Role: - Access to assigned applications and projects - Create, read, update, delete assigned applications and projects</p> <p>Readonly Role: - Read-only access to applications and projects - View applications and projects only</p>"},{"location":"infrastructure/provisioning/#what-gets-created","title":"What Gets Created","text":"<p>When you create a user, the following resources are created:</p> <ol> <li>ArgoCD Account: User account configured in ArgoCD with login capability</li> <li>ArgoCD RBAC: Role-based access control policies for ArgoCD</li> </ol>"},{"location":"infrastructure/provisioning/#after-user-creation","title":"After User Creation","text":"<p>After creating a user, restart the ArgoCD server pod to apply the login changes:</p> <pre><code>kubectl delete pod -n argocd -l app.kubernetes.io/name=argocd-server\n</code></pre>"},{"location":"infrastructure/provisioning/#managing-users","title":"Managing Users","text":"<p>Update user permissions:</p> <pre><code># Update user role\nphd_update_argo_user john.doe --role admin\n\n# Update user password\nphd_update_argo_user john.doe --password \"new-password\"\n</code></pre> <p>Delete a user:</p> <pre><code># Delete user (will prompt for confirmation)\nphd_delete_argo_user john.doe\n\n# Force delete without confirmation\nphd_delete_argo_user john.doe --force\n</code></pre>"},{"location":"infrastructure/provisioning/#user-access","title":"User Access","text":"<p>Users can access ArgoCD via:</p> <ul> <li>ArgoCD Web UI: <code>https://argocd.cluster.domain</code> (or your cluster domain)</li> </ul>"},{"location":"infrastructure/provisioning/#configuration","title":"Configuration","text":""},{"location":"infrastructure/provisioning/#environment-variables","title":"Environment Variables","text":"<p>Required: - <code>LAUNCHPAD_CLUSTER_DOMAIN</code>: Cluster domain name (e.g., <code>cluster.domain</code>)</p> <p>Optional:</p> <ul> <li><code>LAUNCHPAD_ARGO_ADMIN_PASSWORD</code>: Admin password for ArgoCD and Argo Workflows (auto-generated if not set)</li> <li><code>LAUNCHPAD_ARGOCD_VERSION</code>: ArgoCD version (default: <code>stable</code>)</li> <li><code>LAUNCHPAD_ARGO_WORKFLOWS_VERSION</code>: Argo Workflows version (default: <code>stable</code>)</li> <li><code>LAUNCHPAD_OPENCRAFT_MANIFESTS_URL</code>: Base URL for OpenCraft manifests (default: GitHub raw content URL)</li> <li><code>LAUNCHPAD_DOCKER_REGISTRY</code>: Docker registry hostname (default: <code>ghcr.io</code>)</li> <li><code>LAUNCHPAD_DOCKER_REGISTRY_CREDENTIALS</code>: Base64-encoded registry credentials for private image pulls</li> </ul>"},{"location":"infrastructure/provisioning/#terraform-variables","title":"Terraform Variables","text":"<p>The infrastructure modules require various variables. See the <code>variables.tf</code> file in your infrastructure directory for a complete list. Common variables include:</p> <ul> <li>Cloud provider credentials</li> <li>Cluster name and domain</li> <li>Resource sizing (node pools, database instances)</li> <li>Monitoring and backup configuration</li> <li>Network configuration</li> </ul>"},{"location":"infrastructure/provisioning/#troubleshooting","title":"Troubleshooting","text":""},{"location":"infrastructure/provisioning/#cluster-configuration-issues","title":"Cluster Configuration Issues","text":"<p>Template Generation Fails:</p> <ul> <li>Ensure cookiecutter is installed (see Prerequisites)</li> <li>Check that the template repository is accessible</li> <li>Review error messages for missing required variables</li> </ul> <p>Missing Configuration Files:</p> <ul> <li>Ensure you're in the correct directory after cluster creation</li> <li>Verify all required files were generated in the cluster directory</li> </ul>"},{"location":"infrastructure/provisioning/#infrastructure-deployment-issues","title":"Infrastructure Deployment Issues","text":"<p>Terraform/OpenTofu Errors:</p> <ul> <li>Backend Connection: Verify backend credentials are set correctly</li> <li>Provider Credentials: Ensure cloud provider credentials are valid</li> <li>Resource Limits: Check if you've hit cloud provider quotas</li> <li>Network Issues: Verify network connectivity to cloud provider APIs</li> </ul> <p>Common Solutions:</p> <pre><code># Re-initialize if backend configuration changed\ntofu init -backend-config=backend.hcl -reconfigure\n\n# Check Terraform state\ntofu state list\n\n# Validate configuration\ntofu validate\n</code></pre> <p>Kubernetes Cluster Not Accessible:</p> <ul> <li>Verify the cluster was created successfully in your cloud provider console</li> <li>Check that kubeconfig was generated correctly</li> <li>Ensure network connectivity to the cluster API server</li> <li>Check that DNS records are pointing to the Kubernetes cluster</li> </ul>"},{"location":"infrastructure/provisioning/#argocdargo-workflows-installation-issues","title":"ArgoCD/Argo Workflows Installation Issues","text":"<p>Installation Fails:</p> <ul> <li>kubectl Access: Verify <code>kubectl</code> can access the cluster</li> <li>Namespace Conflicts: Check if namespaces already exist</li> <li>Manifest URLs: Verify manifest URLs are accessible</li> <li>Resource Quotas: Ensure cluster has sufficient resources</li> </ul> <p>Check Installation Status:</p> <pre><code># Check ArgoCD installation\nkubectl get pods -n argocd\nkubectl logs -n argocd -l app.kubernetes.io/name=argocd-server\n\n# Check Argo Workflows installation\nkubectl get pods -n argo\nkubectl logs -n argo -l app=argo-server\n</code></pre> <p>UI Not Accessible:</p> <ul> <li>Ingress Issues: Check ingress controller is running</li> <li>DNS Configuration: Verify DNS records point to the ingress</li> <li>TLS Certificates: Check Let's Encrypt certificate status</li> <li>Firewall Rules: Ensure ports 80/443 are accessible</li> </ul> <p>Workflow Templates Not Found:</p> <pre><code># Verify templates were installed\nkubectl get clusterworkflowtemplates\n\n# Re-install templates\nphd_install_argo --workflows-only\n</code></pre> <p>Password Issues:</p> <ul> <li>If password was auto-generated, check the CLI output for the generated password</li> <li>To set a custom password: <code>export LAUNCHPAD_ARGO_ADMIN_PASSWORD=\"your-password\"</code></li> <li>To reset password, re-run <code>phd_install_argo</code> with a new password</li> </ul>"},{"location":"infrastructure/provisioning/#registry-credentials-issues","title":"Registry Credentials Issues","text":"<p>Private Image Pull Failures:</p> <ul> <li>Verify <code>LAUNCHPAD_DOCKER_REGISTRY_CREDENTIALS</code> is set correctly (base64-encoded)</li> <li>Check that credentials are valid and have appropriate permissions</li> <li>Verify secrets were created in namespaces:</li> </ul> <pre><code>kubectl get secrets -n argocd\nkubectl get secrets -n argo\n</code></pre>"},{"location":"infrastructure/provisioning/#next-steps","title":"Next Steps","text":"<p>After successfully provisioning infrastructure:</p> <ol> <li>Configure ArgoCD Projects: Set up projects for each environment</li> <li>Create First Instance: Use <code>phd_create_instance</code> to create your first Open edX instance</li> <li>Set Up Monitoring: Configure monitoring and alerting if not done during infrastructure deployment</li> <li>Backup Configuration: Set up backup schedules if Velero was installed</li> </ol> <p>See the Instance Provisioning documentation for creating Open edX instances.</p>"},{"location":"infrastructure/provisioning/#related-documentation","title":"Related Documentation","text":"<ul> <li>Infrastructure Overview -  Core components (Kubernetes, ArgoCD, Argo Workflows)</li> <li>Deprovisioning -  Removing cluster infrastructure</li> <li>Cluster Overview -  Post-provisioning cluster operations</li> <li>Instance Provisioning -  Creating Open edX instances</li> </ul>"},{"location":"infrastructure/provisioning/#see-also","title":"See Also","text":"<ul> <li>Cluster Authentication -  kubeconfig and Argo users</li> <li>Cluster Configuration -  Environment variables and Terraform</li> <li>Custom Resources -  Workflow templates and manifests</li> </ul>"},{"location":"instances/","title":"Overview","text":""},{"location":"instances/#instances-overview","title":"Instances Overview","text":"<p>Open edX instances run in dedicated Kubernetes namespaces and are managed via ArgoCD and Tutor-generated manifests. Each instance has its own configuration, databases, and storage.</p>"},{"location":"instances/#topics","title":"Topics","text":"<ul> <li>Provisioning -  Creating a new instance (databases, storage, namespace, ArgoCD application)</li> <li>Deprovisioning -  Deleting an instance and cleaning up resources</li> <li>Configuration -  Instance config files (e.g. <code>config.yml</code>), Tutor settings, and secrets</li> <li>Docker Images -  Building and publishing images with Picasso</li> <li>Auto-scaling -  Scaling instance workloads</li> <li>Tracking Logs -  Accessing and following instance logs</li> <li>Logging -  Log aggregation and storage</li> <li>Monitoring -  Metrics and health checks for instances</li> <li>Debugging -  Common issues and troubleshooting</li> </ul>"},{"location":"instances/#lifecycle","title":"Lifecycle","text":"<ol> <li>Create -  <code>phd_create_instance</code> or GitHub Actions \u201cCreate Instance\u201d workflow</li> <li>Build -  Trigger image builds (e.g. openedx, MFE) via Picasso/GitHub Actions</li> <li>Deploy -  ArgoCD syncs the instance application from the cluster repo</li> <li>Operate -  Configure, scale, monitor, and debug as needed</li> <li>Delete -  <code>phd_delete_instance</code> or \u201cDelete Instance\u201d workflow when the instance is no longer needed</li> </ol>"},{"location":"instances/#related-documentation","title":"Related Documentation","text":"<ul> <li>Introduction -  Documentation home</li> <li>Infrastructure Overview -  Cluster components (ArgoCD, Argo Workflows, Tutor, Picasso, Drydock)</li> <li>Provisioning -  Creating a new instance</li> <li>Deprovisioning -  Deleting an instance</li> </ul>"},{"location":"instances/#see-also","title":"See Also","text":"<ul> <li>Configuration -  Instance config and manifests</li> <li>Docker Images -  Building images with Picasso</li> <li>Cluster Overview -  Cluster operations</li> <li>User Guides Overview -  Task-based guides</li> </ul>"},{"location":"instances/auto-scaling/","title":"Auto-scaling","text":""},{"location":"instances/auto-scaling/#auto-scaling","title":"Auto-scaling","text":"<p>This page describes how to scale Open edX instance workloads in the cluster.</p>"},{"location":"instances/auto-scaling/#overview","title":"Overview","text":"<p>Scaling can be applied at several levels:</p> <ul> <li>Kubernetes Deployments -  Increase replicas for LMS, Celery workers, or other services by changing the replica count in the manifests (or via HPA).</li> <li>Horizontal Pod Autoscaler (HPA) -  Automatically scale the number of pods based on CPU, memory, or custom metrics.</li> <li>Cluster autoscaling -  Node pool autoscaling (configured in Terraform/cloud provider) adds nodes when pods are unschedulable.</li> </ul>"},{"location":"instances/auto-scaling/#instance-level-scaling","title":"Instance-level Scaling","text":"<p>Tutor-generated manifests define Deployments for each service. To scale:</p> <ol> <li>Edit the manifest in the cluster repo (e.g. set <code>replicas</code> on the LMS or Celery deployment).</li> <li>Commit and push; ArgoCD will sync the change, or trigger a manual sync.</li> </ol> <p>Alternatively, use <code>kubectl</code> for a one-off change (will be overwritten on next ArgoCD sync):</p> <pre><code>kubectl scale deployment -n &lt;instance-name&gt; &lt;deployment-name&gt; --replicas=&lt;n&gt;\n</code></pre>"},{"location":"instances/auto-scaling/#hpa","title":"HPA","text":"<p>To add autoscaling based on metrics:</p> <ol> <li>Define an HPA resource in the instance manifests (or apply it separately), targeting the desired Deployment and metric (e.g. CPU utilization).</li> <li>Ensure a metrics pipeline (e.g. Prometheus + metrics-server) is available so the HPA controller can read metrics.</li> </ol> <p>Details depend on your cluster\u2019s metrics setup; see Cluster Monitoring.</p> <p>The easiest way to generate the HPA configuration is to use tutor-contrib-pod-autoscaling. The plugin can be configured from the <code>config.yaml</code> of the instance and installed as:</p> <pre><code>PICASSO_EXTRA_COMMANDS:\n  - pip install tutor-contrib-pod-autoscaling\n</code></pre>"},{"location":"instances/auto-scaling/#related-documentation","title":"Related Documentation","text":"<ul> <li>Instances Overview -  Instance lifecycle</li> <li>Configuration -  Where instance manifests live</li> <li>Cluster Monitoring -  Metrics and Prometheus</li> <li>Instance Monitoring -  Instance metrics and health</li> </ul>"},{"location":"instances/auto-scaling/#see-also","title":"See Also","text":"<ul> <li>Provisioning -  Instance layout</li> <li>Infrastructure Overview -  Cluster components</li> <li>Cluster Configuration -  Terraform and node pools</li> </ul>"},{"location":"instances/configuration/","title":"Configuration","text":""},{"location":"instances/configuration/#instance-configuration","title":"Instance Configuration","text":"<p>Instance behavior is controlled by configuration files in the cluster repository and by Tutor-generated Kubernetes manifests.</p>"},{"location":"instances/configuration/#configuration-files","title":"Configuration Files","text":"<p>Each instance has a directory under <code>instances/&lt;instance-name&gt;/</code> in the cluster repo, typically containing:</p> <ul> <li><code>config.yml</code> -  Tutor/Open edX configuration. Used by Tutor to generate Kubernetes manifests and by Picasso for image builds. Contains settings for: Docker images and registry; Tutor version and plugins (e.g. Drydock); Open edX version and repository; LMS/CMS hosts and HTTPS; MySQL, MongoDB, and storage connection details</li> <li><code>application.yml</code> -  ArgoCD Application manifest that points ArgoCD at the source (e.g. cluster repo path) for this instance.</li> </ul> <p>Picasso may overwrite parts of <code>config.yml</code> (e.g. image tags) when building images; avoid hand-editing those sections if you use automated builds.</p>"},{"location":"instances/configuration/#tutor-and-drydock","title":"Tutor and Drydock","text":"<ul> <li>Tutor generates the Kubernetes YAML for the instance (Deployments, Services, Ingress, etc.) from <code>config.yml</code>.</li> <li>Drydock is a Tutor plugin used in this stack; options such as <code>DRYDOCK_INIT_JOBS</code> and <code>DRYDOCK_REGISTRY_CREDENTIALS</code> are set in <code>config.yml</code>.</li> </ul> <p>Regenerate manifests with Tutor from the instance config and commit the result to the cluster repo so ArgoCD can sync.</p>"},{"location":"instances/configuration/#secrets-and-sensitive-data","title":"Secrets and Sensitive Data","text":"<p>Database passwords and similar values will be:</p> <ul> <li>Generated at instance creation (e.g. stored in <code>config.yml</code>)</li> <li>Injected via environment variables in CI (e.g. GitHub Actions secrets) when running <code>phd_create_instance</code></li> </ul> <p>Keep <code>config.yml</code> and any files containing secrets in a private repository and restrict access.</p>"},{"location":"instances/configuration/#related-documentation","title":"Related Documentation","text":"<ul> <li>Instances Overview -  Instance lifecycle</li> <li>Provisioning -  How instance config is generated at creation</li> <li>Infrastructure Overview -  Tutor and Drydock</li> <li>Docker Images -  How config affects image builds</li> </ul>"},{"location":"instances/configuration/#see-also","title":"See Also","text":"<ul> <li>Deprovisioning -  Cleanup and config</li> <li>Debugging -  Config and secrets troubleshooting</li> <li>Cluster Configuration -  Cluster-wide settings</li> </ul>"},{"location":"instances/debugging/","title":"Debugging","text":""},{"location":"instances/debugging/#instance-debugging","title":"Instance Debugging","text":"<p>This page summarizes common issues and debugging steps for Open edX instances on the cluster.</p>"},{"location":"instances/debugging/#pod-not-starting-or-crashing","title":"Pod Not Starting or Crashing","text":"<ol> <li>Describe the pod -  <code>kubectl describe pod -n &lt;instance-name&gt; &lt;pod-name&gt;</code> for events, restarts, and probe failures.</li> <li>Logs -  <code>kubectl logs -n &lt;instance-name&gt; &lt;pod-name&gt; --previous</code> if the container has restarted.</li> <li>Image pull -  Ensure the image exists in the registry and that <code>imagePullSecrets</code> are set (Launchpad configures registry credentials for the namespace).</li> <li>Init jobs -  Drydock init jobs (e.g. migrations) run before the main app; check init container logs if the main container never starts.</li> </ol>"},{"location":"instances/debugging/#argocd-out-of-sync","title":"ArgoCD Out of Sync","text":"<ul> <li>In ArgoCD UI, check the application status and diff.</li> <li>Common causes: manifest errors, missing ConfigMaps/Secrets, or source repo not updated.</li> <li>Fix manifests in the cluster repo and sync, or resolve reported errors (e.g. missing image, wrong namespace).</li> </ul>"},{"location":"instances/debugging/#database-or-storage-connectivity","title":"Database or Storage Connectivity","text":"<ul> <li>Verify Secrets in the instance namespace contain the correct credentials (e.g. MySQL, MongoDB, S3).</li> <li>Check that provision workflows completed and that the instance config (and Tutor env) point to the right hosts and bucket names.</li> <li>Test connectivity from a pod: <code>kubectl exec -n &lt;instance-name&gt; &lt;pod-name&gt; -- ...</code> and run a simple client (e.g. <code>mysql</code>, <code>mongosh</code>) or curl to the storage endpoint.</li> </ul>"},{"location":"instances/debugging/#image-not-found-or-pull-errors","title":"Image Not Found or Pull Errors","text":"<ul> <li>Confirm the image name and tag in <code>config.yml</code> match what was built and pushed (e.g. by Picasso).</li> <li>Ensure the namespace has the correct <code>imagePullSecrets</code> (Launchpad sets this on instance creation and during <code>phd_install_argo</code>).</li> <li>Check registry credentials (e.g. <code>LAUNCHPAD_DOCKER_REGISTRY_CREDENTIALS</code>) in the environment that created the instance or runs the workflows.</li> </ul>"},{"location":"instances/debugging/#slow-or-failing-sync","title":"Slow or Failing Sync","text":"<ul> <li>Check ArgoCD application sync status and any error messages.</li> <li>Ensure the source path in the Application manifest exists and has valid Kubernetes YAML.</li> <li>For large repos, sync can take longer; check ArgoCD logs if needed.</li> </ul>"},{"location":"instances/debugging/#getting-more-help","title":"Getting More Help","text":"<ul> <li>Collect: <code>kubectl describe</code> and <code>kubectl logs</code> for failing pods, ArgoCD application status, and relevant config snippets.</li> <li>See Provisioning and Deprovisioning for workflow-related issues.</li> <li>See Configuration for config and secrets.</li> </ul>"},{"location":"instances/debugging/#related-documentation","title":"Related Documentation","text":"<ul> <li>Instances Overview -  Instance lifecycle</li> <li>Provisioning -  Instance creation and workflows</li> <li>Deprovisioning -  Instance deletion and cleanup</li> <li>Configuration -  Config and secrets</li> </ul>"},{"location":"instances/debugging/#see-also","title":"See Also","text":"<ul> <li>Tracking Logs -  Viewing pod logs</li> <li>Instance Monitoring -  Metrics and health</li> <li>Infrastructure Overview -  ArgoCD and Argo Workflows</li> <li>User Guides Overview -  Task-based guides</li> </ul>"},{"location":"instances/deprovisioning/","title":"Deprovisioning","text":""},{"location":"instances/deprovisioning/#instance-deprovisioning","title":"Instance Deprovisioning","text":"<p>Deprovisioning removes all resources associated with an Open edX instance when it is no longer needed. This includes deleting databases, storage buckets, Kubernetes namespaces, and cleaning up RBAC policies. The deprovisioning process is automated through Argo Workflows and executed when deleting an instance.</p> <p>The deprovisioning system handles:</p> <ul> <li>MySQL Database: Removes database and user</li> <li>MongoDB Database: Deletes databases and user</li> <li>Storage Buckets: Deletes S3-compatible storage buckets and their contents</li> <li>Kubernetes Resources: Removes namespaces, RBAC policies, and service accounts</li> <li>ArgoCD Applications: Deletes the ArgoCD Application managing the instance</li> </ul> <p>All deprovisioning workflows run in parallel. The process is designed to be idempotent, meaning it can be safely retried if it fails partway through.</p>"},{"location":"instances/deprovisioning/#deprovisioning-steps","title":"Deprovisioning Steps","text":""},{"location":"instances/deprovisioning/#prerequisites","title":"Prerequisites","text":"<p>Before deprovisioning an instance, ensure:</p> <ol> <li>Backup Important Data: Deprovisioning permanently deletes all instance data. Ensure you have backups if needed</li> <li>Database Access: Admin credentials for MySQL and MongoDB servers</li> <li>Storage Credentials: Access keys for S3-compatible storage</li> </ol>"},{"location":"instances/deprovisioning/#deprovisioning-process","title":"Deprovisioning Process","text":"<p>The deprovisioning process is automatically executed when deleting an instance using the <code>phd_delete_instance</code> command or the GitHub Actions workflow. The process follows these steps:</p>"},{"location":"instances/deprovisioning/#1-provision-workflow-cleanup","title":"1. Provision Workflow Cleanup","text":"<p>Any remaining provision workflows are deleted to clean up resources.</p>"},{"location":"instances/deprovisioning/#2-deprovision-workflow-creation","title":"2. Deprovision Workflow Creation","text":"<p>Three deprovision workflows are created in parallel:</p> <ul> <li>MySQL Deprovision Workflow: Removes the MySQL database and user</li> <li>MongoDB Deprovision Workflow: Deletes MongoDB databases and user</li> <li>Storage Deprovision Workflow: Deletes the storage bucket and its contents</li> </ul> <p>Each workflow is parameterized with instance-specific configuration values extracted from the instance configuration file.</p>"},{"location":"instances/deprovisioning/#3-workflow-execution","title":"3. Workflow Execution","text":"<p>The workflows execute the following operations:</p> <p>MySQL Deprovisioning: - Connects to the MySQL server using admin credentials - Drops the database specified in the instance configuration - Removes the user associated with the instance - Cleans up any related permissions</p> <p>MongoDB Deprovisioning: - Detects the MongoDB provider - Drops the main database and forum database - Removes the user associated with the instance - For API-based providers, uses the provider's API to manage user deletion</p> <p>Storage Deprovisioning: - Deletes all objects in the storage bucket - Removes the storage bucket itself - Uses force deletion to ensure the bucket is removed even if it contains objects</p>"},{"location":"instances/deprovisioning/#4-workflow-completion","title":"4. Workflow Completion","text":"<p>The system waits for all workflows to complete (default timeout: 300 seconds). Unlike provisioning, deprovisioning workflows that fail are logged as warnings but do not abort the entire process, as some resources may have already been deleted.</p>"},{"location":"instances/deprovisioning/#5-argocd-application-deletion","title":"5. ArgoCD Application Deletion","text":"<p>The ArgoCD Application managing the instance is deleted. This stops any ongoing deployments and removes the application from ArgoCD.</p>"},{"location":"instances/deprovisioning/#6-rbac-cleanup","title":"6. RBAC Cleanup","text":"<p>Cluster-level RBAC resources are removed: - ClusterRole and ClusterRoleBinding for the instance - Any other instance-specific RBAC policies</p>"},{"location":"instances/deprovisioning/#7-namespace-deletion","title":"7. Namespace Deletion","text":"<p>The Kubernetes namespace containing all instance resources is deleted. This operation has a timeout of 300 seconds and will remove all remaining resources in the namespace.</p>"},{"location":"instances/deprovisioning/#8-instance-directory-removal","title":"8. Instance Directory Removal","text":"<p>The local instance configuration directory is removed from the cluster repository.</p>"},{"location":"instances/deprovisioning/#manual-deprovisioning","title":"Manual Deprovisioning","text":"<p>If you need to manually trigger deprovisioning workflows, you can use <code>kubectl</code>:</p> <pre><code># Apply a deprovision workflow manually\nkubectl apply -f &lt;workflow-manifest-url&gt; \\\n  --namespace &lt;instance-name&gt;\n\n# Monitor workflow status\nkubectl get workflows -n &lt;instance-name&gt;\n\n# View workflow logs\nkubectl logs -n &lt;instance-name&gt; \\\n  workflow/&lt;workflow-name&gt;\n</code></pre>"},{"location":"instances/deprovisioning/#force-deletion","title":"Force Deletion","text":"<p>If normal deletion fails, you can use the <code>--force</code> flag to skip confirmation prompts:</p> <pre><code>phd_delete_instance &lt;instance-name&gt; --force\n</code></pre> <p>Warning: Force deletion bypasses safety checks and should only be used when you are certain you want to delete the instance.</p>"},{"location":"instances/deprovisioning/#troubleshooting","title":"Troubleshooting","text":""},{"location":"instances/deprovisioning/#workflow-failures","title":"Workflow Failures","text":"<p>If a deprovisioning workflow fails, check the workflow status and logs:</p> <pre><code># Check workflow status\nkubectl get workflows -n &lt;instance-name&gt;\n\n# View detailed workflow information\nkubectl describe workflow &lt;workflow-name&gt; -n &lt;instance-name&gt;\n\n# View workflow logs\nkubectl logs -n &lt;instance-name&gt; workflow/&lt;workflow-name&gt;\n</code></pre> <p>Common Issues:</p> <ul> <li>Database Connection Failures: Verify that database host, port, and credentials are still valid</li> <li>Resource Already Deleted: Some workflows may fail if resources were already deleted. This is expected and typically safe to ignore</li> <li>Permission Errors: Ensure admin credentials have sufficient privileges to delete databases and users</li> <li>Provider API Errors: For MongoDB Atlas or DigitalOcean, verify API credentials are still valid</li> </ul>"},{"location":"instances/deprovisioning/#namespace-stuck-in-terminating-state","title":"Namespace Stuck in Terminating State","text":"<p>If a namespace is stuck in the \"Terminating\" state:</p> <ol> <li> <p>Check for Finalizers: Some resources may have finalizers preventing deletion    </p><pre><code>kubectl get namespace &lt;instance-name&gt; -o yaml\n</code></pre><p></p> </li> <li> <p>Force Remove Finalizers: If safe, you can manually remove finalizers:    </p><pre><code>kubectl patch namespace &lt;instance-name&gt; \\\n  -p '{\"metadata\":{\"finalizers\":[]}}' \\\n  --type=merge\n</code></pre><p></p> </li> <li> <p>Check Remaining Resources: List resources that may be preventing deletion:    </p><pre><code>kubectl api-resources --verbs=list --namespaced -o name | \\\n  xargs -n 1 kubectl get --show-kind --ignore-not-found -n &lt;instance-name&gt;\n</code></pre><p></p> </li> </ol>"},{"location":"instances/deprovisioning/#partial-deprovisioning","title":"Partial Deprovisioning","text":"<p>If deprovisioning partially succeeds (some resources deleted, others remain):</p> <ol> <li>Identify Remaining Resources: Check what resources still exist</li> <li>Manual Cleanup: Manually delete remaining resources if safe to do so</li> <li>Retry Workflows: Re-run the deprovision workflows for failed components</li> </ol>"},{"location":"instances/deprovisioning/#database-deletion-issues","title":"Database Deletion Issues","text":"<p>If databases cannot be deleted:</p> <ul> <li> <p>MySQL: Ensure no active connections to the database. You may need to manually drop the database:   </p><pre><code>DROP DATABASE IF EXISTS &lt;database-name&gt;;\nDROP USER IF EXISTS '&lt;username&gt;'@'%';\n</code></pre><p></p> </li> <li> <p>MongoDB: For API-based providers, check provider-specific limitations. Some providers may require manual cleanup through their web interface</p> </li> </ul>"},{"location":"instances/deprovisioning/#storage-bucket-deletion-issues","title":"Storage Bucket Deletion Issues","text":"<p>If storage buckets cannot be deleted:</p> <ul> <li>Non-Empty Buckets: Ensure all objects are deleted before deleting the bucket. The deprovision workflow uses force deletion, but some providers may have additional requirements</li> <li>Bucket Policies: Check if bucket policies or lifecycle rules are preventing deletion</li> <li>Manual Cleanup: You may need to manually delete the bucket through the provider's console if the workflow fails</li> </ul>"},{"location":"instances/deprovisioning/#argocd-application-issues","title":"ArgoCD Application Issues","text":"<p>If the ArgoCD Application cannot be deleted:</p> <pre><code># Check application status\nkubectl get application -n argocd &lt;application-name&gt;\n\n# Force delete if necessary\nkubectl delete application &lt;application-name&gt; -n argocd --force --grace-period=0\n</code></pre>"},{"location":"instances/deprovisioning/#rbac-cleanup-issues","title":"RBAC Cleanup Issues","text":"<p>If RBAC resources cannot be deleted:</p> <pre><code># List remaining RBAC resources\nkubectl get clusterrole | grep &lt;instance-name&gt;\nkubectl get clusterrolebinding | grep &lt;instance-name&gt;\n\n# Manually delete if needed\nkubectl delete clusterrole &lt;instance-name&gt;-workflows\nkubectl delete clusterrolebinding &lt;instance-name&gt;-binding\n</code></pre>"},{"location":"instances/deprovisioning/#workflow-timeout","title":"Workflow Timeout","text":"<p>If workflows are timing out:</p> <ol> <li>Check Resource State: Verify that resources are in a state that allows deletion</li> <li>Increase Timeout: The default timeout is 300 seconds. For large databases or buckets with many objects, this may need to be increased</li> <li>Manual Intervention: If workflows consistently timeout, consider manually deleting resources and then cleaning up the workflows</li> </ol>"},{"location":"instances/deprovisioning/#data-recovery","title":"Data Recovery","text":"<p>If you need to recover data after deprovisioning:</p> <ul> <li>Backups: Check if backups were created before deprovisioning (if Velero backup solutions are configured)</li> <li>Database Snapshots: Some database providers maintain snapshots that can be restored</li> <li>Storage Buckets: If the bucket was not force-deleted, objects may still be recoverable depending on the provider's retention policies</li> </ul> <p>Note: Once deprovisioning is complete, data recovery may not be possible. Always ensure backups are created before deprovisioning production instances.</p>"},{"location":"instances/deprovisioning/#getting-help","title":"Getting Help","text":"<p>If deprovisioning continues to fail:</p> <ol> <li>Collect Logs: Gather workflow logs, Kubernetes events, and any error messages</li> <li>Check Resource State: Verify the current state of databases, storage, and Kubernetes resources</li> <li>Review Configuration: Ensure all environment variables and credentials are still valid</li> <li>Manual Cleanup: As a last resort, manually clean up remaining resources following provider-specific procedures</li> </ol>"},{"location":"instances/deprovisioning/#related-documentation","title":"Related Documentation","text":"<ul> <li>Instances Overview -  Instance lifecycle</li> <li>Provisioning -  What gets created for an instance</li> <li>Infrastructure Deprovisioning -  Cluster-level deprovisioning</li> <li>Cluster Backup -  Backing up before deprovisioning</li> </ul>"},{"location":"instances/deprovisioning/#see-also","title":"See Also","text":"<ul> <li>Configuration -  Instance config and credentials</li> <li>Infrastructure Overview -  Argo Workflows and resources</li> <li>Cluster Restore -  Restore procedures</li> <li>Debugging -  Troubleshooting deprovisioning issues</li> </ul>"},{"location":"instances/docker-images/","title":"Docker Images","text":""},{"location":"instances/docker-images/#docker-images","title":"Docker Images","text":"<p>Open edX instances use container images for the LMS/CMS (openedx), MFEs, and other services. These images are built with Picasso and stored in a container registry (e.g. GHCR).</p>"},{"location":"instances/docker-images/#overview","title":"Overview","text":"<ul> <li>Picasso provides GitHub Actions workflows to build Docker images from the instance configuration and Tutor/Picasso setup.</li> <li>Image names and tags are typically set in the instance <code>config.yml</code> (e.g. <code>DOCKER_IMAGE_OPENEDX</code>, <code>MFE_DOCKER_IMAGE</code>) and automatically updated by Picasso after a build.</li> <li>Builds are not automatic on config change; they are triggered manually or by your automation (e.g. \u201cBuild Image\u201d workflow in the cluster repo or the PR sandbox automation).</li> </ul>"},{"location":"instances/docker-images/#building-images","title":"Building Images","text":"<ol> <li>In the cluster repository, use the \u201cBuild Image\u201d workflow.</li> <li>Provide inputs such as instance name, service (<code>openedx</code>, <code>mfe</code>), branch, and Picasso version.</li> <li>The workflow runs the Picasso build, pushes the image to the configured registry, and updates the image tag in the instance <code>config.yml</code>.</li> </ol> <p>Ensure <code>LAUNCHPAD_DOCKER_REGISTRY_CREDENTIALS</code> is set in the environment so the cluster can pull private images.</p>"},{"location":"instances/docker-images/#image-sources","title":"Image Sources","text":"<ul> <li>Open edX core - Built from the repository and version specified in config (e.g. <code>EDX_PLATFORM_REPOSITORY</code>, <code>EDX_PLATFORM_VERSION</code>).</li> <li>MFE - Built according to the Picasso/MFE build steps configured for the instance.</li> <li>Plugins - Tutor plugins (e.g. Drydock, forum, mfe) are installed during the image build as defined in <code>PICASSO_EXTRA_COMMANDS</code> in <code>config.yml</code>.</li> </ul>"},{"location":"instances/docker-images/#modify-files-inside-lms-or-cms-pod-and-see-changes-without-redeployment","title":"Modify files inside LMS or CMS pod and see changes without redeployment","text":"<p>Sometimes, to debug issues in live instances, we need to make changes to files inside the pod and see the changes without redeployment. This can be done in any LMS or CMS pod by following these steps:</p> <ol> <li>Install an editor in the pod to edit files; <code>apt-get</code> does not work due to permission issues. You can use pyvim, a pure Python Vim clone, which is available on GitHub. Run <code>pip install pyvim</code> to install it.</li> <li>Edit any file inside the pod, for example, <code>/openedx/edx-platform/lms/envs/common.py</code>, using pyvim.</li> <li>Restart the uwsgi process by running <code>kill -HUP 1</code>, then refresh your browser to see the changes live.</li> </ol> <p>Any changes made to files in a pod will be lost in the next deployment. This method should only be used for debugging issues.</p>"},{"location":"instances/docker-images/#related-documentation","title":"Related Documentation","text":"<ul> <li>Instances Overview - Instance lifecycle</li> <li>Infrastructure Overview - Picasso and Drydock</li> <li>Configuration - Instance config and image names</li> <li>Provisioning - How config is generated for a new instance</li> </ul>"},{"location":"instances/docker-images/#see-also","title":"See Also","text":"<ul> <li>Deprovisioning - Instance cleanup</li> <li>Debugging - Image pull and tag issues</li> <li>User Guides: Pull Request Sandboxes - Building from PR branches</li> </ul>"},{"location":"instances/logging/","title":"Logging","text":""},{"location":"instances/logging/#logging","title":"Logging","text":"<p>This page outlines logging options for Open edX instances on the cluster.</p>"},{"location":"instances/logging/#kubernetes-logs","title":"Kubernetes Logs","text":"<p>By default, container stdout/stderr are captured by the container runtime and can be viewed with <code>kubectl logs</code>.</p> <p>Log retention is typically limited by node disk and cluster configuration; old logs may be rotated away.</p>"},{"location":"instances/logging/#application-level-logging","title":"Application-level Logging","text":"<p>Open edX services write logs to stdout. Log level and format are controlled by application settings (e.g. Django <code>LOGGING</code>, Celery, or environment variables). Adjust these in the instance configuration or in custom settings if you need different verbosity or structure.</p> <p>To get the application logs, use:</p> <pre><code>kubectl logs deployments/&lt;service&gt; -n &lt;instance-name&gt;\n</code></pre>"},{"location":"instances/logging/#related-documentation","title":"Related Documentation","text":"<ul> <li>Instances Overview -  Instance lifecycle</li> <li>Tracking Logs -  Using <code>kubectl logs</code> and following logs</li> <li>Cluster Monitoring -  Grafana and Prometheus</li> <li>Debugging -  Using logs for troubleshooting</li> </ul>"},{"location":"instances/logging/#see-also","title":"See Also","text":"<ul> <li>Instance Monitoring -  Metrics and health</li> <li>Configuration -  Application log settings</li> <li>Infrastructure Overview -  Cluster components</li> </ul>"},{"location":"instances/monitoring/","title":"Monitoring","text":""},{"location":"instances/monitoring/#instance-monitoring","title":"Instance Monitoring","text":"<p>This page describes monitoring options for Open edX instances running on the cluster.</p>"},{"location":"instances/monitoring/#overview","title":"Overview","text":"<p>Instance health and metrics can be observed via:</p> <ul> <li>Kubernetes -  Pod status, restarts, resource usage</li> <li>Prometheus -  If the cluster has Prometheus and scrape configs for instance namespaces</li> <li>Grafana -  Dashboards built on Prometheus (or other datasources)</li> <li>Application health -  LMS/CMS endpoints, Celery workers, and custom checks</li> </ul>"},{"location":"instances/monitoring/#kubernetes","title":"Kubernetes","text":"<p>Basic health:</p> <pre><code>kubectl get pods -n &lt;instance-name&gt;\nkubectl describe pod -n &lt;instance-name&gt; &lt;pod-name&gt;\nkubectl top pods -n &lt;instance-name&gt;\n</code></pre> <p>Check events and readiness/liveness probe status in <code>kubectl describe</code>.</p>"},{"location":"instances/monitoring/#prometheus-and-grafana","title":"Prometheus and Grafana","text":"<p>If the cluster has Prometheus and Grafana enabled (see Cluster Monitoring):</p> <ul> <li>Ensure ServiceMonitors or Prometheus scrape configs include the instance namespace and relevant services.</li> <li>Use or create Grafana dashboards for Open edX (e.g. request rate, latency, Celery queue length, Redis).</li> </ul> <p>Metrics depend on what is exposed (e.g. Django metrics, Redis, Celery) and how Prometheus is configured to scrape them.</p>"},{"location":"instances/monitoring/#alerts","title":"Alerts","text":"<p>Define alerts in Alertmanager (or your alerting system) for:</p> <ul> <li>Pod crashes or not ready</li> <li>High error rate or latency</li> <li>Queue backlog (Celery)</li> <li>Database or Redis connectivity</li> </ul> <p>Configure receivers (email, Slack, etc.) in the cluster\u2019s Alertmanager configuration.</p>"},{"location":"instances/monitoring/#related-documentation","title":"Related Documentation","text":"<ul> <li>Instances Overview -  Instance lifecycle</li> <li>Cluster Monitoring -  Cluster-level Prometheus and Grafana</li> <li>Configuration -  Instance and app settings</li> <li>Debugging -  Troubleshooting instance issues</li> </ul>"},{"location":"instances/monitoring/#see-also","title":"See Also","text":"<ul> <li>Tracking Logs -  Accessing logs</li> <li>Logging -  Log aggregation</li> <li>Infrastructure Overview -  Harmony and monitoring</li> <li>Auto-scaling -  HPA and metrics</li> </ul>"},{"location":"instances/provisioning/","title":"Provisioning","text":""},{"location":"instances/provisioning/#instance-provisioning","title":"Instance Provisioning","text":"<p>Provisioning creates and configures the necessary resources for a new Open edX instance. This includes setting up databases and users for MySQL and MongoDB, storage buckets, Kubernetes namespaces, and RBAC policies. The provisioning process is automated through Argo Workflows and executed when creating a new instance.</p> <p>The provisioning system handles:</p> <ul> <li>MySQL Database: Creates database and user with appropriate permissions</li> <li>MongoDB Database: Creates databases and user with appropriate permissions</li> <li>Storage Buckets: Creates S3-compatible storage buckets for media files and static assets</li> <li>Kubernetes Resources: Sets up namespaces, RBAC policies, and service accounts</li> </ul> <p>All provisioning workflows run in parallel and must complete successfully before the instance can be built and deployed.</p>"},{"location":"instances/provisioning/#provisioning-steps","title":"Provisioning Steps","text":""},{"location":"instances/provisioning/#prerequisites","title":"Prerequisites","text":"<p>Before provisioning an instance, ensure the following are configured:</p> <ol> <li>Kubernetes Cluster: A running Kubernetes cluster with ArgoCD and Argo Workflows installed</li> <li>Database Access: Admin credentials for MySQL and MongoDB servers</li> <li>Storage Credentials: Access keys for S3-compatible storage (AWS S3 or DigitalOcean Spaces)</li> <li>Environment Variables: Required configuration variables set (see below)</li> </ol>"},{"location":"instances/provisioning/#required-environment-variables","title":"Required Environment Variables","text":"<p>Docker Registry: </p><pre><code>export LAUNCHPAD_DOCKER_REGISTRY_CREDENTIALS=\"base64-encoded user:password\"\n</code></pre><p></p> <p>MySQL Database: </p><pre><code>export LAUNCHPAD_MYSQL_HOST=\"mysql.cluster.domain\"\nexport LAUNCHPAD_MYSQL_PORT=\"3306\"\nexport LAUNCHPAD_MYSQL_ADMIN_USER=\"root\"\nexport LAUNCHPAD_MYSQL_ADMIN_PASSWORD=\"secure_password\"\n</code></pre><p></p> <p>MongoDB Database (DigitalOcean): </p><pre><code>export LAUNCHPAD_MONGODB_HOST=\"mongodb.cluster.domain\"\nexport LAUNCHPAD_MONGODB_PORT=\"27017\"\nexport LAUNCHPAD_MONGODB_ADMIN_USER=\"admin\"\nexport LAUNCHPAD_MONGODB_ADMIN_PASSWORD=\"secure_password\"\nexport LAUNCHPAD_MONGODB_CLUSTER_ID=\"abc12345-xyz67890\"\nexport LAUNCHPAD_MONGODB_AUTH_SOURCE=\"admin\"\nexport LAUNCHPAD_DIGITALOCEAN_TOKEN=\"dop_v1_your_token\"\n</code></pre><p></p> <p>MongoDB Database (MongoDB Atlas): </p><pre><code>export LAUNCHPAD_ATLAS_PUBLIC_KEY=\"your_public_key\"\nexport LAUNCHPAD_ATLAS_PRIVATE_KEY=\"your_private_key\"\nexport LAUNCHPAD_ATLAS_PROJECT_ID=\"your_project_id\"\nexport LAUNCHPAD_ATLAS_CLUSTER_NAME=\"Cluster0\"\n</code></pre><p></p> <p>Storage: </p><pre><code>export LAUNCHPAD_STORAGE_TYPE=\"spaces\"  # or \"s3\"\nexport LAUNCHPAD_STORAGE_REGION=\"nyc3\"  # or \"us-east-1\"\nexport LAUNCHPAD_STORAGE_ACCESS_KEY_ID=\"your_key\"\nexport LAUNCHPAD_STORAGE_SECRET_ACCESS_KEY=\"your_secret\"\n</code></pre><p></p>"},{"location":"instances/provisioning/#provisioning-process","title":"Provisioning Process","text":"<p>The provisioning process is automatically executed when creating a new instance using the <code>phd_create_instance</code> command or the GitHub Actions workflow (recommended). The process follows these steps:</p>"},{"location":"instances/provisioning/#1-namespace-and-rbac-setup","title":"1. Namespace and RBAC Setup","text":"<ul> <li>Creates a Kubernetes namespace for the instance</li> <li>Configures RBAC policies to allow Argo Workflows to manage resources</li> <li>Sets up service accounts and permissions</li> </ul>"},{"location":"instances/provisioning/#2-workflow-creation","title":"2. Workflow Creation","text":"<p>Three provision workflows are created in parallel:</p> <ul> <li>MySQL Provision Workflow: Creates the MySQL database and user</li> <li>MongoDB Provision Workflow: Creates MongoDB databases and user</li> <li>Storage Provision Workflow: Creates the storage bucket</li> </ul> <p>Each workflow is parameterized with instance-specific configuration values extracted from the instance configuration file.</p>"},{"location":"instances/provisioning/#3-workflow-execution","title":"3. Workflow Execution","text":"<p>The workflows execute the following operations:</p> <p>MySQL Provisioning: - Connects to the MySQL server using admin credentials - Creates the database specified in the instance configuration - Creates a user with appropriate permissions</p> <p>MongoDB Provisioning: - Detects the MongoDB provider (DigitalOcean API, Atlas, or direct connection) - Creates the main database and forum database - Creates a user with appropriate permissions - For API-based providers, uses the provider's API to manage users</p> <p>Storage Provisioning: - Creates an S3-compatible storage bucket - Configures bucket permissions (private by default)</p>"},{"location":"instances/provisioning/#4-workflow-completion","title":"4. Workflow Completion","text":"<p>The system waits for all workflows to complete successfully. If any workflow fails, the provisioning process is aborted and an error is reported.</p>"},{"location":"instances/provisioning/#5-configuration-updates","title":"5. Configuration Updates","text":"<p>After successful provisioning: - MongoDB password is retrieved from the Kubernetes Secret and updated in the instance configuration file - The temporary secret containing the MongoDB password is deleted for security</p>"},{"location":"instances/provisioning/#6-argocd-application-creation","title":"6. ArgoCD Application Creation","text":"<p>Once provisioning is complete, an ArgoCD Application is created to manage the Open edX deployment. The application monitors the cluster repository and deploys the instance when changes are detected.</p>"},{"location":"instances/provisioning/#troubleshooting","title":"Troubleshooting","text":""},{"location":"instances/provisioning/#workflow-failures","title":"Workflow Failures","text":"<p>If a provisioning workflow fails, check the workflow status and logs:</p> <pre><code># Check workflow status\nkubectl get workflows -n &lt;instance-name&gt;\n\n# View detailed workflow information\nkubectl describe workflow &lt;workflow-name&gt; -n &lt;instance-name&gt;\n\n# View workflow logs\nkubectl logs -n &lt;instance-name&gt; workflow/&lt;workflow-name&gt;\n</code></pre> <p>Common Issues:</p> <ul> <li>Database Connection Failures: Verify that database host, port, and credentials are correct</li> <li>Permission Errors: Ensure admin credentials have sufficient privileges to create databases and users</li> <li>Network Issues: Check that the Kubernetes cluster can reach the database servers</li> <li>Provider API Errors: For MongoDB Atlas or DigitalOcean, verify API credentials and permissions</li> </ul>"},{"location":"instances/provisioning/#partial-provisioning","title":"Partial Provisioning","text":"<p>If provisioning partially succeeds (some workflows succeed, others fail):</p> <ol> <li>Clean Up: Delete any partially created resources</li> <li>Check Dependencies: Ensure all required services (databases, storage) are available</li> <li>Retry: Re-run the instance creation process, which will attempt to provision all resources again</li> </ol>"},{"location":"instances/provisioning/#getting-help","title":"Getting Help","text":"<p>If provisioning continues to fail:</p> <ol> <li>Collect Logs: Gather workflow logs, Kubernetes events, and any error messages</li> <li>Check Configuration: Verify all environment variables and instance configuration values</li> <li>Check Cluster Health: Ensure the Kubernetes cluster and Argo Workflows are functioning correctly</li> </ol>"},{"location":"instances/provisioning/#related-documentation","title":"Related Documentation","text":"<ul> <li>Instances Overview -  Instance lifecycle</li> <li>Infrastructure Provisioning -  Cluster and ArgoCD setup</li> <li>Deprovisioning -  Deleting an instance</li> <li>Configuration -  Instance config generated at creation</li> </ul>"},{"location":"instances/provisioning/#see-also","title":"See Also","text":"<ul> <li>Docker Images -  Building images after provisioning</li> <li>Infrastructure Overview -  Argo Workflows and provisioning</li> <li>Cluster Authentication -  kubeconfig for running Launchpad CLI</li> <li>Debugging -  Troubleshooting provisioning issues</li> </ul>"},{"location":"instances/tracking-logs/","title":"Tracking Logs","text":""},{"location":"instances/tracking-logs/#tracking-logs","title":"Tracking Logs","text":"<p>To be done.</p>"},{"location":"instances/tracking-logs/#related-documentation","title":"Related Documentation","text":"<ul> <li>Instances Overview -  Instance lifecycle</li> <li>Logging -  Log aggregation and storage</li> <li>Debugging -  Troubleshooting and common issues</li> <li>Instance Monitoring -  Health and metrics</li> </ul>"},{"location":"instances/tracking-logs/#see-also","title":"See Also","text":"<ul> <li>Configuration -  Instance and app settings</li> <li>Cluster Monitoring -  Grafana and Prometheus</li> <li>User Guides Overview -  Task-based guides</li> </ul>"},{"location":"user-guides/","title":"Overview","text":""},{"location":"user-guides/#user-guides-overview","title":"User Guides Overview","text":"<p>This section contains step-by-step guides for specific tasks and scenarios when operating a Launchpad cluster and Open edX instances.</p>"},{"location":"user-guides/#guides","title":"Guides","text":"<ul> <li>Using AWS WAF and ALB -  Configuring AWS WAF and Application Load Balancer with your cluster</li> <li>Instance Maintenance Mode -  Putting an instance into maintenance mode</li> <li>Setting Up Cron Jobs -  Configuring cron jobs for Open edX instances</li> <li>Pull Request Sandboxes -  Using sandbox environments for pull requests</li> <li>Multi-Domain Setup -  Serving an instance under multiple domains</li> <li>Migrating Instances from Grove - Step-by-step checklist to help instance migration</li> </ul>"},{"location":"user-guides/#related-documentation","title":"Related Documentation","text":"<ul> <li>Introduction -  Documentation home</li> <li>Infrastructure Overview -  Cluster components</li> <li>Cluster Overview -  Cluster operations</li> <li>Instances Overview -  Instance lifecycle and operations</li> </ul>"},{"location":"user-guides/#see-also","title":"See Also","text":"<ul> <li>Using AWS WAF and ALB -  AWS WAF and ALB</li> <li>Instance Maintenance Mode -  Maintenance mode</li> <li>Instance Provisioning -  Creating instances</li> <li>Instance Configuration -  Instance config</li> </ul>"},{"location":"user-guides/instance-maintenance-mode/","title":"Instance Maintenance Mode","text":""},{"location":"user-guides/instance-maintenance-mode/#instance-maintenance-mode","title":"Instance Maintenance Mode","text":"<p>Maintenance mode for an instance can be toggled while working locally. Make you're set up to work with instances on you machine before running any of the commands below.</p> <p>Maintenance pages are set up to work from any S3-compatible bucket in order to allow operators to change the content without needing to modify the cluster. The maintenance mode requires tutor-contrib-grove to be set up.</p> <p>To deploy a maintenance page:</p> <ol> <li>Create an S3-compatible bucket and make sure that it's not publicly readable. In the bucket place a file, named <code>maintenance-mode.html</code>. When maintenance mode is enabled, this file will be served instead of your LMS or Studio pages.</li> <li>Once done, add the configuration <code>GROVE_MAINTENANCE_S3_BUCKET_ROOT_URL</code> to your <code>config.yml</code>. The value should be your bucket's fully qualified URL (eg. <code>https://grove-maintenance.ams3.digitaloceanspaces.com/</code>).</li> <li>Maintenance mode can then be enable with the command: <code>./tutor [instance-name] maintenance-mode --enable</code>, executed from the <code>./control</code> directory.</li> <li>Disabling maintenance mode is similar and can be accomplished by executing <code>./tutor [instance-name] maintenance-mode --disable</code> within the <code>./control</code> directory.</li> </ol>"},{"location":"user-guides/instance-maintenance-mode/#404-pages","title":"404 pages","text":"<p>When an instance is being provisioned for the first time, this page will display. The default 404 page can be found in <code>provider-modules/harmony/ingress-404.html</code>.</p> <p>To serve a customer file, you may change the <code>TF_VAR_global_404_html_path</code> to point to your HTML file.</p> <p>Note that Grove tools run within Docker, so you'll need to place the file within a path where Grove can find it. By default your <code>my-cluster</code> directory is mounted in the <code>/workspace</code> directory in the Grove container.</p> <p>What this means is, if you place your file (<code>404.html</code>) in the root of your <code>my-cluster</code> repo you can then deploy it as part of Grove by adding the variable to <code>cluster.yml</code> or to you CI/CD vars:</p> <p><code>TF_VAR_global_404_html_path=\"/workspace/404.html\"</code> Redeploy the changes. If you try and access an instance that has not yet been provisioned, the HTML file will be rendered.</p> <p>When your if your instance is ready, this page will not be rendered for any 404's, but rather the Open edX platform will render an appropriately styled page.</p> <p>Note</p> <p>If your changes do not reflect, you might need to restart the container that serves these pages:</p> <pre><code>./kubectl rollout restart deployment -nkube-system ingress-nginx-defaultbackend\n</code></pre>"},{"location":"user-guides/instance-maintenance-mode/#5xx-error-pages","title":"5xx error pages","text":"<p>These pages work like the maintenance pages in that they have to point to an S3 bucket.</p> <p>In order to enable custom error pages, just set the <code>GROVE_SERVER_ERROR_S3_BUCKET_ROOT_URL</code> to your S3 bucket's URL. Once added, the file <code>server-error.html</code> will be displayed for any 5xx error that cannot be handled by the LMS/CMS.</p>"},{"location":"user-guides/instance-maintenance-mode/#related-documentation","title":"Related Documentation","text":"<ul> <li>User Guides Overview -  All user guides</li> <li>Instance Configuration -  Config and manifests</li> <li>Instance Provisioning -  Instance layout and ArgoCD</li> <li>Instances Overview -  Instance lifecycle</li> </ul>"},{"location":"user-guides/instance-maintenance-mode/#see-also","title":"See Also","text":"<ul> <li>Auto-scaling -  Scaling replicas</li> <li>Cluster Overview -  Cluster operations</li> <li>Debugging -  Troubleshooting</li> </ul>"},{"location":"user-guides/migrating-instances-from-grove/","title":"Migrating Instances from Grove","text":""},{"location":"user-guides/migrating-instances-from-grove/#migrating-instances-from-grove","title":"Migrating Instances from Grove","text":""},{"location":"user-guides/migrating-instances-from-grove/#configure-the-new-instance","title":"Configure the new instance","text":"<p>-[ ] set the DNS record TTL to 300s -[ ] create a new instance in the new cluster using the \"create instance\" GitHub workflow -[ ] make sure the new instance's <code>aplication.yml</code> has <code>spec.syncPolicy.automated.enabled</code> set to <code>false</code> -[ ] create any necessary infrastructure resources for the new instance (used by plugins) -[ ] ensure the old instance configuration (<code>config.yml</code>) is replicated for the new instance -[ ] configure theming and branding for the new instance -[ ] build the new instance using the \"build instance\" GitHub workflow -[ ] deploy the new instance using ArgoCD and wait for it to be ready (~20 minutes for the first deployment) -[ ] check that the new instance looks as expected -[ ] proceed with instance test guide to verify the instance is working as expected -[ ] turn off the init jobs for the new instance by setting <code>DRYDOCK_INIT_JOBS</code> to <code>false</code> in the <code>config.yml</code></p>"},{"location":"user-guides/migrating-instances-from-grove/#point-the-new-instance-to-the-old-instance-data","title":"Point the new instance to the old instance data","text":"<p>We are going to simplify the migration by making the new instance to use the old instance data. This allows us to avoid data migration, but requires careful configuration of the new instance. Both instances will be using the same database and storage for some time!</p> <p>-[ ] get the application configuration from the old instance</p> <pre><code># List all configmaps\n./kubectl -n &lt;instance-name&gt; get cm\n\n# LMS configuration\n./kubectl -n &lt;instance-name&gt; get cm openedx-settings-lms-&lt;latest id&gt; \\\n  -o jsonpath='{.data.production\\.py}' &gt; &lt;instance-name&gt;-settings-lms.py\n\n# CMS configuration\n./kubectl -n &lt;instance-name&gt; get cm openedx-settings-cms-&lt;latest id&gt; \\\n  -o jsonpath='{.data.production\\.py}' &gt; &lt;instance-name&gt;-settings-cms.py\n\n# if other configuration is needed, get it from the old instance\n# -- recommended to get from instance settings using Django shell\n# ./kubectl -n &lt;instance-name&gt; exec -it deployments/lms -- ./manage.py lms shell\n</code></pre> <p>-[ ] update the MySQL, MongoDB, S3 and other configuration in the new instance config to use the old instance data (i.e. the same database and storage) -[ ] enable the new cluster access for old databases (MongoDB and MySQL) -[ ] build the new instance using the \"build instance\" GitHub workflow -[ ] make a backup of the old instance data (in MySQL, MongoDB, S3, etc.) -[ ] deploy the new instance using ArgoCD and wait for it to be ready -[ ] proceed with instance test guide to verify the old instance is still working as expected -[ ] check instance specific configuration and make sure non-generic configuration is working as expected (e.g., LTI, OAuth, etc.)</p>"},{"location":"user-guides/migrating-instances-from-grove/#dns-changes","title":"DNS changes","text":"<p>We need to update the DNS records to point to the new instance. This may lead to a 0-7 minutes outage while the Let's Encrypt certificates are (re-)generated. This section is split into 2 sections: safe and unsafe operations. Proceed in order.</p> <p>Safe operations:</p> <p>-[ ] update the instance config with the expected FQDNs for LMS, CMS, etc. -[ ] build the new instance using the \"build instance\" GitHub workflow</p> <p>Unsafe operations:</p> <p>-[ ] change the DNS record to point to the new cluster, and wait about a minute for DNS propagation (monitor DNS at [https://dnschecker.org/]). For CNAME aliased record, the propagation may take a bit longer. -[ ] (optional -- only if a must) destroy ingress controllers [^1] and TLS secrets [^2] -[ ] (optional -- only if a must) deploy the new instance using ArgoCD and wait for it to be ready -[ ] proceed with instance test guide to verify the new instance is working as expected -[ ] check the logs for errors -[ ] update the DNS record TTL to 3600s once confirmed that the instance is OK</p> <p>[^1]: <code>kubectl -n courses delete ing --all</code> [^2]: <code>kubectl -n courses delete secrets cms-host-tls lms-host-tls meilisearch-host-tls mfe-host-tls</code></p>"},{"location":"user-guides/migrating-instances-from-grove/#enable-argocd-automated-sync","title":"Enable ArgoCD automated sync","text":"<p>-[ ] enable ArgoCD automated sync for the new instance by setting <code>spec.syncPolicy.automated.enabled</code> to <code>true</code> in the <code>application.yml</code></p>"},{"location":"user-guides/migrating-instances-from-grove/#swap-terraform-resources","title":"Swap Terraform resources","text":"<p>IMPORTANT: Do this step if and only if ALL instances are migrated from the cluster!</p> <p>TBD -- This section of the documentation will be finalized when the first cluster is migrated at OpenCraft and we discovered all rabbit holes.</p>"},{"location":"user-guides/migrating-instances-from-grove/#ensure-instance-permissions-are-correct","title":"Ensure instance permissions are correct","text":"<p>The new infrastructure uses proper permissions for the instance resources, but the old infrastructure does not. We need to make sure the instance user has access only to the instance resources and not the entire database cluster(s).</p> <p>The new infrastructure grants:</p> <ul> <li>MySQL: <code>ALL PRIVILEGES</code> on the instance database only (e.g. <code>GRANT ALL PRIVILEGES ON \\</code>phd-instance-openedx`. TO 'phd-instance'@'%'<code>), not on</code>.*`</li> <li>MongoDB: <code>readWrite</code> role on the instance\u2019s main and forum databases only, not <code>readWriteAnyDatabase</code> or other cluster-wide roles</li> </ul>"},{"location":"user-guides/migrating-instances-from-grove/#step-1-identify-instance-database-values","title":"Step 1: Identify instance database values","text":"<p>-[ ] From the migrated instance <code>config.yml</code> or Kubernetes configmaps, note:</p> <ul> <li><code>MYSQL_DATABASE</code> (e.g. <code>phd-instance-openedx</code>)</li> <li><code>MYSQL_USERNAME</code> (e.g. <code>phd-instance</code>)</li> <li><code>MONGODB_DATABASE</code> (e.g. <code>phd-instance-openedx</code>)</li> <li><code>FORUM_MONGODB_DATABASE</code> (e.g. <code>phd-instance-forum</code>)</li> <li><code>MONGODB_USERNAME</code> (e.g. <code>phd-instance</code>)</li> </ul> <p>Note: example values will be used below.</p>"},{"location":"user-guides/migrating-instances-from-grove/#step-2-restrict-mysql-permissions","title":"Step 2: Restrict MySQL permissions","text":"<p>-[ ] If the instance user has global privileges (e.g. on <code>*.*</code>), restrict it to the instance database only.</p> <p>Prerequisites: MySQL admin credentials (root or user with <code>GRANT</code>) -- allow your IP as a trusted source as needed depending on the Cloud Provider.</p> <ol> <li>Connect as admin:</li> </ol> <pre><code>mysql -h &lt;MYSQL_HOST&gt; -P &lt;MYSQL_PORT&gt; -u &lt;ADMIN_USER&gt; -p\n</code></pre> <ol> <li>Inspect current grants:</li> </ol> <pre><code>-- List all users matching the instance username to find the correct host\nSELECT user, host FROM mysql.user WHERE user = 'phd-instance';\nSHOW GRANTS FOR 'phd-instance'@'%';\n</code></pre> <p>Use the actual <code>user@host</code> from <code>mysql.user</code> if it differs from <code>'%'</code>. If you see <code>GRANT ALL PRIVILEGES ON *.*</code>, the user has cluster-wide access and should be restricted.</p> <ol> <li>Revoke global privileges and grant scoped access (replace placeholders; use the actual <code>user@host</code> if it differs from <code>'%'</code>):</li> </ol> <pre><code>-- Revoke global privileges (adjust if the user has different grants)\nREVOKE ALL PRIVILEGES ON *.* FROM 'phd-instance'@'%';\n\n-- Grant access only to the instance database\nGRANT ALL PRIVILEGES ON `phd-instance-openedx`.* TO 'phd-instance'@'%';\n\nFLUSH PRIVILEGES;\n</code></pre> <ol> <li>Confirm the grants:</li> </ol> <pre><code>SHOW GRANTS FOR 'phd-instance'@'%';\n</code></pre> <p>Expected: <code>GRANT ALL PRIVILEGES ON \\</code>phd-instance-openedx`.* TO 'phd-instance'@'%'`.</p>"},{"location":"user-guides/migrating-instances-from-grove/#step-3-restrict-mongodb-permissions","title":"Step 3: Restrict MongoDB permissions","text":"<p>-[ ] Follow the steps for your MongoDB provider:</p>"},{"location":"user-guides/migrating-instances-from-grove/#digitalocean-managed-mongodb-api","title":"DigitalOcean Managed MongoDB (API)","text":"<p>Use the DigitalOcean API to ensure the user has <code>readWrite</code> only on the instance databases.</p> <ol> <li> <p>Get the cluster ID from your DigitalOcean project or infrastructure config.</p> </li> <li> <p>Check the current user (replace <code>CLUSTER_ID</code> and <code>USERNAME</code>):</p> </li> </ol> <pre><code>curl -s -X GET \\\n  -H \"Authorization: Bearer $DIGITALOCEAN_TOKEN\" \\\n  \"https://api.digitalocean.com/v2/databases/CLUSTER_ID/users/USERNAME\"\n</code></pre> <ol> <li>If the user has cluster-wide access, recreate it with scoped databases:</li> </ol> <pre><code># Delete the existing user (note: this invalidates the password; you must update the instance config with the new one)\ncurl -s -X DELETE \\\n  -H \"Authorization: Bearer $DIGITALOCEAN_TOKEN\" \\\n  \"https://api.digitalocean.com/v2/databases/CLUSTER_ID/users/USERNAME\"\n\n# Wait a few seconds, then create the user with scoped access\nsleep 3\ncurl -s -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $DIGITALOCEAN_TOKEN\" \\\n  -d '{\"name\": \"USERNAME\", \"settings\": {\"mongo_user_settings\": {\"databases\": [\"phd-instance-openedx\", \"phd-instance-forum\"], \"role\": \"readWrite\"}}}' \\\n  \"https://api.digitalocean.com/v2/databases/CLUSTER_ID/users\"\n</code></pre> <p>Update the instance <code>config.yml</code> with the new MongoDB password from the API response.</p>"},{"location":"user-guides/migrating-instances-from-grove/#mongodb-atlas","title":"MongoDB Atlas","text":"<p>Use the Atlas UI or CLI to give the user <code>readWrite</code> only on the instance databases.</p> <ol> <li> <p>In Atlas: Project \u2192 Database Access \u2192 edit the user.</p> </li> <li> <p>Ensure the user has roles:</p> </li> </ol> <ul> <li><code>readWrite</code> on the main database (e.g. <code>phd-instance-openedx</code>)</li> <li><code>readWrite</code> on the forum database (e.g. <code>phd-instance-forum</code>)</li> </ul> <ol> <li>Remove any cluster-wide roles such as <code>readWriteAnyDatabase</code> or <code>readAnyDatabase</code>.</li> </ol> <p>Or via Atlas CLI:</p> <pre><code>atlas dbusers update phd-instance \\\n  --role \"readWrite@phd-instance-openedx\" \\\n  --role \"readWrite@phd-instance-forum\" \\\n  --projectId &lt;PROJECT_ID&gt;\n</code></pre>"},{"location":"user-guides/migrating-instances-from-grove/#self-hosted-mongodb-mongo-shell","title":"Self-hosted MongoDB (mongo shell)","text":"<p>Use <code>mongosh</code> (MongoDB 5.0+). On older setups, use <code>mongo</code> if <code>mongosh</code> is not available.</p> <ol> <li>Connect as admin:</li> </ol> <pre><code>mongosh \"mongodb://&lt;ADMIN_USER&gt;:&lt;ADMIN_PASSWORD&gt;@&lt;MONGODB_HOST&gt;:&lt;MONGODB_PORT&gt;/admin?authSource=admin\"\n</code></pre> <ol> <li>Inspect current roles:</li> </ol> <pre><code>use admin\ndb.getUser(\"phd-instance\")\n</code></pre> <ol> <li>Replace roles with scoped <code>readWrite</code> (adjust usernames and database names):</li> </ol> <pre><code>use admin\ndb.updateUser(\"phd-instance\", {\n  roles: [\n    { role: \"readWrite\", db: \"phd-instance-openedx\" },\n    { role: \"readWrite\", db: \"phd-instance-forum\" }\n  ]\n})\n</code></pre> <p>If the user does not exist, create it instead:</p> <pre><code>use admin\ndb.createUser({\n  user: \"phd-instance\",\n  pwd: \"&lt;PASSWORD_FROM_CONFIG&gt;\",\n  roles: [\n    { role: \"readWrite\", db: \"phd-instance-openedx\" },\n    { role: \"readWrite\", db: \"phd-instance-forum\" }\n  ]\n})\n</code></pre>"},{"location":"user-guides/migrating-instances-from-grove/#step-4-verify-the-instance-works","title":"Step 4: Verify the instance works","text":"<p>-[ ] Ensure the instance can connect to MySQL and MongoDB (LMS/CMS healthy) -[ ] Run the instance test guide to confirm expected behaviour</p>"},{"location":"user-guides/multi-domain-setup/","title":"Multi-Domain Setup","text":""},{"location":"user-guides/multi-domain-setup/#multi-domain-setup","title":"Multi-Domain Setup","text":"<p>To be done.</p>"},{"location":"user-guides/multi-domain-setup/#related-documentation","title":"Related Documentation","text":"<ul> <li>User Guides Overview -  All user guides</li> <li>Instance Configuration -  Tutor and hosts</li> <li>Infrastructure Overview -  Ingress and TLS</li> <li>Instances Overview -  Instance lifecycle</li> </ul>"},{"location":"user-guides/multi-domain-setup/#see-also","title":"See Also","text":"<ul> <li>Using AWS WAF and ALB -  ALB and ingress</li> <li>Instance Provisioning -  Instance setup</li> <li>Cluster Configuration -  Cluster settings</li> </ul>"},{"location":"user-guides/pull-request-sandboxes/","title":"Pull Request Sandboxes","text":""},{"location":"user-guides/pull-request-sandboxes/#pull-request-sandboxes","title":"Pull Request Sandboxes","text":"<p>You may want to automatically create sandbox instances for GitHub Pull Requests. To do so, the PR sandbox automation must be installed for the given GitHub organization and configured for the cluster.</p> <p>You can find detailed instructions in the PR sandbox automation's README.</p>"},{"location":"user-guides/pull-request-sandboxes/#related-documentation","title":"Related Documentation","text":"<ul> <li>User Guides Overview -  All user guides</li> <li>Instance Provisioning -  Creating instances</li> <li>Instance Deprovisioning -  Deleting instances</li> <li>Docker Images -  Building from a branch</li> </ul>"},{"location":"user-guides/pull-request-sandboxes/#see-also","title":"See Also","text":"<ul> <li>Instances Overview -  Instance lifecycle</li> <li>Instance Configuration -  Instance config</li> <li>Infrastructure Provisioning -  Cluster setup</li> <li>Instance Maintenance Mode -  Maintenance workflows</li> </ul>"},{"location":"user-guides/setting-up-cron-jobs/","title":"Setting Up Cron Jobs","text":""},{"location":"user-guides/setting-up-cron-jobs/#setting-up-cron-jobs","title":"Setting Up Cron Jobs","text":"<p>This guide describes how to set up cron jobs for Open edX instances on the cluster using CronJob resource for custom tasks.</p>"},{"location":"user-guides/setting-up-cron-jobs/#kubernetes-cronjob","title":"Kubernetes CronJob","text":"<p>For custom cron jobs:</p> <ol> <li>Define a CronJob -  Create a CronJob manifest (schedule, job template, image, command). Use an image that has the required tools (e.g. openedx image or a minimal runner).</li> <li>Place in instance manifests -  Add the CronJob to the instance\u2019s manifest directory so ArgoCD syncs it.</li> <li>Secrets and config -  Mount any required secrets (e.g. DB, API keys) into the CronJob\u2019s job template so the job can connect to services.</li> </ol> <p>Example (conceptual):</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-custom-job\n  namespace: &lt;instance-name&gt;\nspec:\n  schedule: \"0 2 * * *\"   # 2 AM daily\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n            - name: job\n              image: &lt;your-image&gt;\n              command: [\"/path/to/script.sh\"]\n          restartPolicy: OnFailure\n</code></pre>"},{"location":"user-guides/setting-up-cron-jobs/#deploying-with-argocd","title":"Deploying with ArgoCD","text":"<p>CronJobs are deployed via ArgoCD from the cluster repository. The instance\u2019s ArgoCD Application must include the path where the CronJob manifests live, and a sync must be triggered after changes.</p> <ol> <li> <p>Update <code>application.yml</code> so ArgoCD also sees the cron manifests:    </p><pre><code># ...\nspec:\n project: default\n\n sources:\n   - path: \"instances/&lt;instance-name&gt;/env\"\n     repoURL: \"git@github.com:open-craft/&lt;cluster-repo-name&gt;\"\n     targetRevision: \"main\"\n\n   - path: \"instances/&lt;instance-name&gt;/manifests\"\n     repoURL: \"git@github.com:open-craft/&lt;cluster-repo-name&gt;\"\n     targetRevision: \"main\"\n# ...\n</code></pre><p></p> </li> <li> <p>Add the CronJob manifest in the <code>manifests</code> directory. Use the instance\u2019s namespace in the CronJob <code>metadata.namespace</code>.</p> </li> <li> <p>Commit and push the CronJob YAML and any change to <code>application.yml</code> to the cluster repo.</p> </li> <li> <p>Trigger a sync for the instance\u2019s ArgoCD Application (UI or CLI). A sync is required for the new or updated CronJobs to be applied.</p> </li> <li> <p>Verify in the ArgoCD application view or with <code>kubectl get cronjob -n &lt;instance-name&gt;</code>.</p> </li> </ol> <p>Any change to the CronJob (schedule, image, command) should be made in the manifest in the repo and committed; direct edits with <code>kubectl</code> will be reverted on the next ArgoCD sync.</p>"},{"location":"user-guides/setting-up-cron-jobs/#related-documentation","title":"Related Documentation","text":"<ul> <li>User Guides Overview -  All user guides</li> <li>Instance Configuration -  Where manifests live</li> <li>Instances Overview -  Instance lifecycle</li> </ul>"},{"location":"user-guides/setting-up-cron-jobs/#see-also","title":"See Also","text":"<ul> <li>Instance Provisioning -  Instance layout</li> <li>Docker Images -  Custom images for cron</li> <li>Instance Debugging -  Troubleshooting jobs</li> </ul>"},{"location":"user-guides/using-aws-waf-and-alb/","title":"Using AWS WAF and ALB","text":""},{"location":"user-guides/using-aws-waf-and-alb/#using-aws-waf-and-alb","title":"Using AWS WAF and ALB","text":"<p>By default, in AWS, Harmony uses an NGINX ingress. This ingress is backed by AWS ELB load balancers.</p> <p>That ingress setup works for most use cases, but in case you need to use a feature that is only supported by the Application Load Balancer type, such as a WAF, you can configure it with the following setup:</p>"},{"location":"user-guides/using-aws-waf-and-alb/#1-cluster-configuration","title":"1. Cluster Configuration","text":"<p>The ALB ingress controller requires some extra permissions added to the Kubernetes cluster. You can declare a policy that provides those permissions by adding the following statements to a Terraform file inside the <code>infrastructure</code> folder:</p> <pre><code>variable \"cluster_self_managed_node_groups\" {}\n\nresource \"aws_iam_policy\" \"alb_policy\" {\n  name        = \"alb-policy\"\n  description = \"Policy required for alb creation.\"\n  policy      = file(\"${path.module}/alb-role.json\")\n}\n\n# Optional policy that makes it posible to use ALB ingress\nresource \"aws_iam_policy_attachment\" \"alb-policy-attachment\" {\n  name       = \"${var.cluster_name}-alb-attachment\"\n  roles      = [var.cluster_self_managed_node_groups[\"worker_group\"].iam_role_name]\n  policy_arn = aws_iam_policy.alb_policy.arn\n}\n\nresource \"helm_release\" \"alb_controller\" {\n  name       = \"aws-load-balancer-controller\"\n  repository = \"https://aws.github.io/eks-charts\"\n  chart      = \"aws-load-balancer-controller\"\n  version    = \"1.7.2\" # NOTE: set the latest version\n  namespace  = \"kube-system\"\n  depends_on = [\n    aws_iam_policy_attachment.alb-policy-attachment\n  ]\n\n  set {\n    name  = \"clusterName\"\n    value = var.kubernetes_cluster_name\n  }\n\n  set {\n    name  = \"region\"\n    value = var.region\n  }\n\n  set {\n    name  = \"vpcId\"\n    value = var.vpc_id\n  }\n}\n</code></pre> <p>You'll also need to download the iam policy role json definition into the <code>infrastructure/alb-role.json</code> from Kubernetes SIGs aws-load-balancer-controller's repo.</p>"},{"location":"user-guides/using-aws-waf-and-alb/#2-declare-ingress","title":"2. Declare Ingress","text":"<p>Now you can declare an ALB Ingress in the <code>infrastructure</code> folder. Here's an example of one:</p> <pre><code>resource \"kubernetes_manifest\" \"alb_ingress\" {\n  depends_on = [helm_release.alb_controller]\n  manifest   = {\n    \"apiVersion\" = \"networking.k8s.io/v1\"\n    \"kind\" = \"Ingress\"\n    \"metadata\" = {\n      \"annotations\" = {\n        \"alb.ingress.kubernetes.io/scheme\" = \"internet-facing\"\n        \"alb.ingress.kubernetes.io/target-type\": \"ip\"\n      }\n      \"labels\" = {\n        \"app.kubernetes.io/part-of\" = \"openedx\"\n      }\n      \"name\" = \"alb-ingress\"\n      \"namespace\" = \"&lt;INSTANCE-NAMESPACE&gt;\"\n    }\n    \"spec\" = {\n      \"ingressClassName\" = \"alb\"\n      \"rules\" = [\n        # Repeat this rule for all the hostnames you'll point to this ingress.\n        {\n          \"host\" = \"&lt;HOSTNAME&gt;\"\n          \"http\" = {\n            \"paths\" = [\n              {\n                \"backend\" = {\n                  \"service\" = {\n                    \"name\" = \"caddy\"\n                    \"port\" = {\n                      \"number\" = 80\n                    }\n                  }\n                }\n                \"path\" = \"/\"\n                \"pathType\" = \"Prefix\"\n              },\n            ]\n          }\n        }\n      ]\n    }\n  }\n}\n</code></pre> <p>As an alternative, you may use Kubernetes manifests, deployed using ArgoCD.</p>"},{"location":"user-guides/using-aws-waf-and-alb/#3-setup-dns","title":"3. Setup DNS","text":"<p>The last step is adding a CNAME record pointing to the ALB domain for any domain you want to configure. You can find the ALB domain in the AWS console. You may also want to remove any records pointing to the old NGINX Ingress.</p>"},{"location":"user-guides/using-aws-waf-and-alb/#optional-disable-default-ingress","title":"Optional: Disable default Ingress","text":"<p>In order to avoid having double the ammount of Load Balancers, you can disable the default load balancer as needed in the harmony helm values file</p>"},{"location":"user-guides/using-aws-waf-and-alb/#optional-enable-waf","title":"Optional: Enable WAF","text":"<p>An example of a resource that requires ALB is Amazon's Web Application Firewall (WAF). Once you have completed the above setup you can either add a WAF manually to the generated ingress in the AWS console, or declare it in the <code>infrastructure</code> directory with the following terraform code:</p> <pre><code>resource \"aws_wafv2_web_acl\" \"alb_waf\" {\n  name  = \"alb-waf\"\n  scope = \"REGIONAL\"\n\n  default_action {\n    allow {}\n  }\n\n\n  # This is an example rule. Create one based on your needs.\n  rule {\n    name     = \"RateLimit\"\n    priority = 1\n\n    action {\n      block {}\n    }\n\n    statement {\n\n      rate_based_statement {\n        aggregate_key_type = \"IP\"\n        limit              = 500\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = true\n      metric_name                = \"RateLimit\"\n      sampled_requests_enabled   = true\n    }\n  }\n\n  visibility_config {\n    cloudwatch_metrics_enabled = false\n    metric_name                = \"alb-waf\"\n    sampled_requests_enabled   = false\n  }\n}\n</code></pre> <p>You also need to update your ingress tags:</p> <pre><code>resource \"kubernetes_manifest\" \"alb_ingress\" {\n  manifest = {\n    \"apiVersion\" = \"networking.k8s.io/v1\"\n    \"kind\" = \"Ingress\"\n    \"metadata\" = {\n      \"annotations\" = {\n        \"alb.ingress.kubernetes.io/scheme\" = \"internet-facing\"\n        \"alb.ingress.kubernetes.io/target-type\": \"ip\"\n        \"alb.ingress.kubernetes.io/wafv2-acl-arn\": aws_wafv2_web_acl.alb_waf.arn\n</code></pre> <p>NOTE: If you set up WAF manually you need disable the contoller's WAF capabilities by   setting controller command line flags <code>--enable-waf=false</code> or <code>--enable-wafv2=false</code>. If   the controller is also managing WAF, it'll make sure that the annotation matches exactly   the waf acl linked to the Load Balancer. This means that it will delete the waf acl if it   doesn't match with the Ingress annotations.</p>"},{"location":"user-guides/using-aws-waf-and-alb/#related-documentation","title":"Related Documentation","text":"<ul> <li>User Guides Overview -  All user guides</li> <li>Infrastructure Overview -  Ingress and Harmony</li> <li>Cluster Configuration -  Cluster and Terraform settings</li> <li>Instances Overview -  Instance lifecycle</li> </ul>"},{"location":"user-guides/using-aws-waf-and-alb/#see-also","title":"See Also","text":"<ul> <li>Infrastructure Provisioning -  Cluster setup</li> <li>Instance Configuration -  Instance manifests</li> <li>Multi-Domain Setup -  Multiple domains and ingress</li> </ul>"}]}